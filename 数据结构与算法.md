# 数据结构与算法

## 1. 排序

排序算法的执行效率对于排序算法执行效率的分析，我们一般会从这几个方面来衡量：

1. ##### **最好情况、最坏情况、平均情况时间复杂度**

   ​		我们在分析排序算法的时间复杂度时，要分别给出最好情况、最坏情况、平均情况下的时间复杂度。除此之外，你还要说出最好、最坏时间复杂度对应的要排序的原始数据是什么样的。为什么要区分这三种时间复杂度呢？第一，有些排序算法会区分，为了好对比，所以我们最好都做一下区分。第二，对于要排序的数据，有的接近有序，有的完全无序。有序度不同的数据，对于排序的执行时间肯定是有影响的，我们要知道排序算法在不同数据下的性能表现。

2. #####  **时间复杂度的系数、常数 、低阶**

   ​		我们知道，时间复杂度反映的是数据规模 n 很大的时候的一个增长趋势，所以它表示的时候会忽略系数、常数、低阶。但是实际的软件开发中，我们排序的可能是 10 个、100 个、1000 个这样规模很小的数据，所以，在对同一阶时间复杂度的排序算法性能对比的时候，我们就要把系数、常数、低阶也考虑进来。

3. #####  比较次数和交换（或移动）次数

   ​	基于比较的排序算法的执行过程，会涉及两种操作，一种是元素比较大小，另一种是元素交换或移动。所以，如果我们在分析排序算法的执行效率的时候，应该把比较次数和交换（或移动）次数也考虑进去。

- #### 排序算法的内存

  - 消耗我们前面讲过，算法的内存消耗可以通过空间复杂度来衡量，排序算法也不例外。不过，针对排序算法的空间复杂度，我们还引入了一个新的概念，**原地排序（Sorted in place）**。原地排序算法，就是特指空间复杂度是 O(1) 的排序算法。我们今天讲的三种排序算法，都是原地排序算法。

- #### 排序算法的稳定性

  - 仅仅用执行效率和内存消耗来衡量排序算法的好坏是不够的。针对排序算法，我们还有一个重要的度量指标，**稳定性**。这个概念是说，如果待排序的序列中存在值相等的元素，经过排序之后，相等元素之间原有的先后顺序不变。

### 1.1 冒泡排序（Bubble Sort）

​		冒泡排序只会操作相邻的两个数据。每次冒泡操作都会对相邻的两个元素进行比较，看是否满足大小关系要求。如果不满足就让它俩互换。一次冒泡会让至少一个元素移动到它应该在的位置，重复 n 次，就完成了 n 个数据的排序工作。

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\4038f64f47975ab9f519e4f739e464e9.webp" style="zoom:67%;" />

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\9246f12cca22e5d872cbfce302ef4d09.webp" style="zoom:67%;" />

**代码实现**	

```go
package main

import "fmt"

func main() {
	array := [6]int{4, 5, 6, 3, 2, 1}
	BubbleSort(array[:])
	fmt.Println(array[:])
}

func BubbleSort(a []int) {
	len := len(a)
	for i := 0; i < len; i++ {
		// 设置标志位
		flag := false
		for j := 0; j < len-1-i; j++ {
			if a[j] > a[j+1] {
				a[j], a[j+1] = a[j+1], a[j]
				flag = true
			}
			fmt.Println(a[:])
		}
		if !flag {
			break
		}
	}
}
```

- #### 第一，冒泡排序是原地排序算法吗？

  - 冒泡的过程只涉及相邻数据的交换操作，只需要常量级的临时空间，所以它的空间复杂度为 O(1)，**是一个原地排序算法**。

- #### 第二，冒泡排序是稳定的排序算法吗？

  - 在冒泡排序中，只有交换才可以改变两个元素的前后顺序。为了保证冒泡排序算法的稳定性，当有相邻的两个元素大小相等的时候，我们不做交换，相同大小的数据在排序前后不会改变顺序，所以**冒泡排序是稳定的排序算法**。

- #### 第三，冒泡排序的时间复杂度是多少？

  - 最好情况下，要排序的数据已经是有序的了，我们只需要进行一次冒泡操作，就可以结束了，所以最好情况时间复杂度是 O(n)。而最坏的情况是，要排序的数据刚好是倒序排列的，我们需要进行 n 次冒泡操作，所以最坏情况时间复杂度为 O(n^2)。



### 1.2 插入排序（Insertion Sort）

​	动态地往有序集合中添加数据，我们可以通过这种方法保持集合中的数据一直有序。而对于一组静态数据，我们也可以借鉴上面讲的插入方法，来进行排序，于是就有了插入排序算法。

​		一个有序的数组，我们往里面添加一个新的数据后，如何继续保持数据有序呢？很简单，我们只要遍历数组，找到数据应该插入的位置将其插入即可。

​		**插入算法的核心思想是取未排序区间中的元素，在已排序区间中找到合适的插入位置将其插入，并保证已排序区间数据一直有序。重复这个过程，直到未排序区间中元素为空，算法结束。**	

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\插入排序1.webp" style="zoom: 67%;" />

插入排序也包含两种操作，一种是元素的比较，一种是元素的移动。当我们需要将一个数据 a 插入到已排序区间时，需要拿 a 与已排序区间的元素依次比较大小，找到合适的插入位置。找到插入点之后，我们还需要将插入点之后的元素顺序往后移动一位，这样才能腾出位置给元素 a 插入。

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\插入排序2.webp" style="zoom: 67%;" />

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\插入排序3.webp" style="zoom: 67%;" />

```go
package main

import "fmt"

func main() {
	array := [6]int{4, 5, 6, 1, 3, 2}
	InsertSort(array[:])
	fmt.Println(array[:])
}

func InsertSort(array []int) {
	length := len(array)
	for i := 0; i < length; i++ {
		tmp := array[i]
		j := i - 1
		for ; j >= 0; j-- {
			if array[j] > tmp {
				array[j+1] = array[j]
			} else {
				break
			}
			array[j] = tmp
		}
	}
}
```



- #### 第一，插入排序是原地排序算法吗？

  从实现过程可以很明显地看出，插入排序算法的运行并不需要额外的存储空间，所以空间复杂度是 O(1)，也就是说，这是一个原地排序算法。

- #### 第二，插入排序是稳定的排序算法吗？

  在插入排序中，对于值相同的元素，我们可以选择将后面出现的元素，插入到前面出现元素的后面，这样就可以保持原有的前后顺序不变，所以插入排序是稳定的排序算法。

- #### 第三，插入排序的时间复杂度是多少？

  如果要排序的数据已经是有序的，我们并不需要搬移任何数据。如果我们从尾到头在有序数据组里面查找插入位置，每次只需要比较一个数据就能确定插入的位置。所以这种情况下，最好是时间复杂度为 O(n)。注意，这里是从尾到头遍历已经有序的数据。如果数组是倒序的，每次插入都相当于在数组的第一个位置插入新的数据，所以需要移动大量的数据，所以最坏情况时间复杂度为 O(n^2)。还记得我们在数组中插入一个数据的平均时间复杂度是多少吗？没错，是 O(n)。所以，对于插入排序来说，每次插入操作都相当于在数组中插入一个数据，循环执行 n 次插入操作，所以平均时间复杂度为 O(n^2)。



### 1.3 选择排序（Selection Sort）

选择排序算法的实现思路有点类似插入排序，也分已排序区间和未排序区间。但是选择排序每次会从未排序区间中找到最小的元素，将其放到已排序区间的末尾	<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\选择排序.webp" alt="选择排序" style="zoom: 67%;" />             



选择排序空间复杂度为 O(1)，是一种原地排序算法。`选择排序的最好情况时间复杂度`、最坏情况和平均情况时间复杂度都为 O(n^2)。

不是稳定的排序算法。

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\算法比较.webp" style="zoom: 50%;" />



### 1.4 归并排序 (Merge Sort）

​		归并排序的核心思想还是蛮简单的。如果要排序一个数组，我们先把数组从中间分成前后两部分，然后对前后两部分分别排序，再将排好序的两部分合并在一起，这样整个数组就都有序了。

​		总的来讲就是使用递归的思想：

```go
递推公式：
merge_sort(p…r) = merge(merge_sort(p…q), merge_sort(q+1…r))

终止条件：
p >= r 不用再继续分解
```

我们申请一个临时数组 `tmp`，大小与 A[p...r]相同。我们用两个游标 i 和 j，分别指向 A[p...q]和 A[q+1...r]的第一个元素。比较这两个元素 A[i]和 A[j]，如果 A[i]<=A[j]，我们就把 A[i]放入到临时数组 `tmp`，并且 i 后移一位，否则将 A[j]放入到数组 tmp，j 后移一位。继续上述比较过程，直到其中一个子数组中的所有数据都放入临时数组中，再把另一个数组中的数据依次加入到临时数组的末尾，这个时候，临时数组中存储的就是两个子数组合并之后的结果了。最后再把临时数组 `tmp` 中的数据拷贝到原数组 A[p...r]中。

继续上述比较过程，直到其中一个子数组中的所有数据都放入临时数组中，再把另一个数组中的数据依次加入到临时数组的末尾，这个时候，临时数组中存储的就是两个子数组合并之后的结果了。最后再把临时数组 tmp 中的数据拷贝到原数组 A[p...r]中

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\归并排序.webp" style="zoom: 50%;" />

代码：

```go
// 归并排序
func MergeSort(array []int) {
	length := len(array)
	mergeSort(array, 0, length-1)
}

func mergeSort(array []int, start, end int) {
	if start >= end {
		return
	}
	mid := (start + end) / 2
	mergeSort(array, start, mid)
	mergeSort(array, mid+1, end)
	merge(array, start, mid, end)
}

func merge(array []int, start, mid, end int) {
	tmpArr := make([]int, end-start+1)
	i := start
	j := mid + 1
	k := 0

	for ; i <= mid && j <= end; k++ {
		if array[i] > array[j] {
			tmpArr[k] = array[j]
			j++
		} else {
			tmpArr[k] = array[i]
			i++
		}
	}

	for ; i <= mid; i++ {
		tmpArr[k] = array[i]
		k++
	}

	for ; j <= end; j++ {
		tmpArr[k] = array[j]
		k++
	}

	copy(array[start:end+1], tmpArr)
}
```

### 1.5 快速排序 (Quick Sort)

快速排序的思想也比较简单，就是选取一个标准值，将整个数组划分为两个部分，一个部分中所有的值都比标准值大另一个部分每个值都比标准值小，对两个部分再递归的进行快速排序，直到全部结束<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\快速排序.webp" style="zoom: 80%;" />

代码：

```go
// 快速排序
func QuickSort(array []int, left, right int) {
	if left < right {
		loc := Paration(array, left, right)
		QuickSort(array, left, loc-1)
		QuickSort(array, loc+1, right)
	}
}

func Paration(array []int, left, right int) int {
	i := left + 1
	j := right
	for i < j {
		if array[left] < array[i] {
			array[i], array[j] = array[j], array[i]
			j--
		} else {
			i++
		}
	}
	if array[i] >= array[left] {
		i--
	}
	array[i], array[left] = array[left], array[i]
	return i
}
```

**快排是一种原地、不稳定的排序算法，时间复杂度O(nlogn)**，



### 1.6 桶排序 (Bucket sort)

顾名思义，会用到“桶”，核心思想是将要排序的数据分到几个有序的桶里，每个桶里的数据再单独进行排序。桶内排完序之后，再把每个桶里的数据按照顺序依次取出，组成的序列就是有序的了。

如果要排序的数据有 n 个，我们把它们均匀地划分到 m 个桶内，每个桶里就有 k=n/m 个元素。每个桶内部使用快速排序，时间复杂度为 O(k * logk)。m 个桶排序的时间复杂度就是 O(m * k * logk)，因为 k=n/m，所以整个桶排序的时间复杂度就是 O(n*log(n/m))。当桶的个数 m 接近数据个数 n 时，log(n/m) 就是一个非常小的常量，这个时候桶排序的时间复杂度接近 O(n)。



**桶排序比较适合用在外部排序中**。所谓的外部排序就是数据存储在外部磁盘中，数据量比较大，内存有限，无法将数据全部加载到内存中。比如说我们有 10GB 的订单数据，我们希望按订单金额（假设金额都是正整数）进行排序，但是我们的内存有限，只有几百 MB，没办法一次性把 10GB 的数据都加载到内存中。这个时候该怎么办呢？



### 1.7 计数排序 (Counting sort)

**计数排序其实是桶排序的一种特殊情况。**当要排序的 n 个数据，所处的范围并不大的时候，比如最大值是 k，我们就可以把数据划分成 k 个桶。每个桶内的数据值都是相同的，省掉了桶内排序的时间。

计数排序比较适合数据范围比较小，或者数据量远远大于数据范围时候

```go
// 计数排序
func CountingSort(a []int) {
	n := len(a)
	if n <= 1 {
		return
	}
	// 确定计数数组长度
	var max int = math.MinInt32
	for _, v := range a {
		if v > max {
			max = v
		}
	}
	// 初始化计数数组，向其中填入值
	c := make([]int, max+1)
	for i := range a {
		c[a[i]]++
	}
	// 对数组进行累加
	for i := 1; i <= max; i++ {
		c[i] += c[i-1]
	}

	//初始化一个新数组r
	r := make([]int, n)
	for i := n - 1; i >= 0; i-- {
		index := c[a[i]] - 1
		r[index] = a[i]
		c[a[i]]--
	}
	copy(a, r)
}
```



## 2. 查找

### 2.1 二分查找

二分查找针对的是一个有序的数据集合，查找思想有点类似分治思想。每次都通过跟区间的中间元素对比，将待查找的区间缩小为之前的一半，直到找到要查找的元素，或者区间被缩小为 0。

![img](https://static001.geekbang.org/resource/image/d1/94/d1e4fa1542e187184c87c545c2fe4794.jpg?wh=1142*337)

所以时间复杂度就是 O(logn)。

- #### 二分查找的循环实现

  ```go
  func BinanSearch(arr []int, x int) int {
  	length := len(arr)
  	if length == 0 {
  		return -1
  	}
  	low := 0
  	high := length - 1
  	for low <= high {
  		mid := (low + high) / 2
  		if arr[mid] == x {
  			return mid
  		} else if arr[mid] < x {
  			low = mid + 1
  		} else {
  			high = mid - 1
  		}
  	}
  	return -1
  }
  ```



- #### 二分查找的递归实现

  ```go
  func BinaryDigui(arr []int, x int) int {
  	length := len(arr)
  	return bs(arr, x, 0, length-1)
  }
  func bs(arr []int, x, low, high int) int {
  	if low > high {
  		return -1
  	}
  	mid := (low + high) / 2
  	if arr[mid] == x {
  		return mid
  	} else if arr[mid] < x {
  		return bs(arr, x, mid+1, high)
  	} else {
  		return bs(arr, x, low, mid-1)
  	}
  }
  ```

  

- #### 二分查找的变形

  ```go
  // 二分查找：查找第一个第一个值等于给定值的元素
  func BinarySearchFirstEqual(arr []int, x int) int {
  	length := len(arr)
  	if length == 0 {
  		return -1
  	}
  	low := 0
  	high := length - 1
  	for low <= high {
  		mid := (low + high) >> 1
  		if arr[mid] < x {
  			low = mid + 1
  		} else if arr[mid] > x {
  			high = mid - 1
  		} else {
  			if mid == 0 || arr[mid-1] != x {
  				return mid
  			} else {
  				high = mid - 1
  			}
  		}
  	}
  	return -1
  }
  
  // 二分查找：查找最后一个值等于给定值的元素
  func BinarySearchLastEqual(arr []int, x int) int {
  	length := len(arr)
  	if length == 0 {
  		return -1
  	}
  	low := 0
  	high := length - 1
  	for low <= high {
  		mid := (low + high) >> 1
  		if arr[mid] < x {
  			low = mid + 1
  		} else if arr[mid] > x {
  			high = mid - 1
  		} else {
  			if mid == length-1 || arr[mid+1] != x {
  				return mid
  			} else {
  				low = mid + 1
  			}
  		}
  	}
  	return -1
  }
  
  // 二分查找：查找第一个值大于等于给定值的元素
  func BinarySearchFirstBigger(arr []int, x int) int {
  	length := len(arr)
  	low := 0
  	high := length - 1
  	for low <= high {
  		mid := low + (high-low)>>1
  		if arr[mid] < x {
  			low = mid + 1
  		} else {
  			if mid == 0 || arr[mid-1] < x {
  				return mid
  			} else {
  				high = mid - 1
  			}
  		}
  
  	}
  	return -1
  }
  
  // 二分查找：查找最后一个小于等于给定值的元素
  func BinarySearchLastLess(arr []int, x int) int {
  	length := len(arr) - 1
  	low := 0
  	high := length - 1
  	for low <= high {
  		mid := low + (high-low)>>1
  		if arr[mid] > x {
  			high = mid - 1
  		} else {
  			if mid == length-1 || arr[mid+1] > x {
  				return mid
  			} else {
  				low = mid + 1
  			}
  		}
  	}
  	return -1
  }
  
  ```



### 2.2 跳表

链表加多级索引的结构，就是跳表。

<img src="https://static001.geekbang.org/resource/image/46/a9/46d283cd82c987153b3fe0c76dfba8a9.jpg?wh=1142*636" alt="img" style="zoom:67%;" />

**第 k 级索引的结点个数是第 k-1 级索引的结点个数的 1/2，那第 k级索引结点的个数就是 n/(2k)。**

假设我们要查找的数据是 x，在第 k 级索引中，我们遍历到 y 结点之后，发现 x 大于 y，小于后面的结点 z，所以我们通过 y 的 down 指针，从第 k 级索引下降到第 k-1 级索引。在第 k-1 级索引中，y 和 z 之间只有 3 个结点（包含 y 和 z），所以，我们在 K-1 级索引中最多只需要遍历 3 个结点，依次类推，每一级索引都最多只需要遍历 3 个结点。

<img src="https://static001.geekbang.org/resource/image/d0/0c/d03bef9a64a0368e6a0d23ace8bd450c.jpg?wh=1142*439" alt="img" style="zoom:67%;" />

单链表中，一旦定位好要插入的位置，插入结点的时间复杂度是很低的，就是 O(1)。但是，这里为了保证原始链表中数据的有序性，我们需要先找到要插入的位置，这个查找操作就会比较耗时。对于纯粹的单链表，需要遍历每个结点，来找到插入的位置。但是，对于跳表来说，我们讲过查找某个结点的时间复杂度是 O(logn)，所以这里查找某个数据应该插入的位置，方法也是类似的，时间复杂度也是 O(logn)。我画了一张图，你可以很清晰地看到插入的过程。好了，我们再来看删除操作。如果这个结点在索引中也有出现，我们除了要删除原始链表中的结点，还要删除索引中的。因为单链表中的删除操作需要拿到要删除结点的前驱结点，然后通过指针操作完成删除。所以在查找要删除的结点的时候，一定要获取前驱结点。当然，如果我们用的是双向链表，就不需要考虑这个问题了。

- ### 跳表

  ```go
  package main
  
  import (
  	"fmt"
  	"math"
  	"math/rand"
  )
  
  const (
  	//最高层数
  	MAX_LEVEL = 16
  )
  
  //跳表节点结构体
  type skipListNode struct {
  	//跳表保存的值
  	v interface{}
  	//用于排序的分值
  	score int
  	//层高
  	level int
  	//每层前进指针
  	forwards []*skipListNode
  }
  
  //新建跳表节点
  func newSkipListNode(v interface{}, score, level int) *skipListNode {
  	return &skipListNode{v: v, score: score, forwards: make([]*skipListNode, level, level), level: level}
  }
  
  //跳表结构体
  type SkipList struct {
  	//跳表头结点
  	head *skipListNode
  	//跳表当前层数
  	level int
  	//跳表长度
  	length int
  }
  
  //实例化跳表对象
  func NewSkipList() *SkipList {
  	//头结点，便于操作
  	head := newSkipListNode(0, math.MinInt32, MAX_LEVEL)
  	return &SkipList{head, 1, 0}
  }
  
  //获取跳表长度
  func (sl *SkipList) Length() int {
  	return sl.length
  }
  
  //获取跳表层级
  func (sl *SkipList) Level() int {
  	return sl.level
  }
  
  //插入节点到跳表中
  func (sl *SkipList) Insert(v interface{}, score int) int {
  	if nil == v {
  		return 1
  	}
  
  	//查找插入位置
  	cur := sl.head
  	//记录每层的路径
  	update := [MAX_LEVEL]*skipListNode{}
  	i := MAX_LEVEL - 1
  	for ; i >= 0; i-- {
  		for nil != cur.forwards[i] {
  			if cur.forwards[i].v == v {
  				return 2
  			}
  			if cur.forwards[i].score > score {
  				update[i] = cur
  				break
  			}
  			cur = cur.forwards[i]
  		}
  		if nil == cur.forwards[i] {
  			update[i] = cur
  		}
  	}
  
  	//通过随机算法获取该节点层数
  	level := 1
  	for i := 1; i < MAX_LEVEL; i++ {
  		if rand.Int31()%7 == 1 {
  			level++
  		}
  	}
  
  	//创建一个新的跳表节点
  	newNode := newSkipListNode(v, score, level)
  
  	//原有节点连接
  	for i := 0; i <= level-1; i++ {
  		next := update[i].forwards[i]
  		update[i].forwards[i] = newNode
  		newNode.forwards[i] = next
  	}
  
  	//如果当前节点的层数大于之前跳表的层数
  	//更新当前跳表层数
  	if level > sl.level {
  		sl.level = level
  	}
  
  	//更新跳表长度
  	sl.length++
  
  	return 0
  }
  
  //查找
  func (sl *SkipList) Find(v interface{}, score int) *skipListNode {
  	if nil == v || sl.length == 0 {
  		return nil
  	}
  
  	cur := sl.head
  	for i := sl.level - 1; i >= 0; i-- {
  		for nil != cur.forwards[i] {
  			if cur.forwards[i].score == score && cur.forwards[i].v == v {
  				return cur.forwards[i]
  			} else if cur.forwards[i].score > score {
  				break
  			}
  			cur = cur.forwards[i]
  		}
  	}
  
  	return nil
  }
  
  //删除节点
  func (sl *SkipList) Delete(v interface{}, score int) int {
  	if nil == v {
  		return 1
  	}
  
  	//查找前驱节点
  	cur := sl.head
  	//记录前驱路径
  	update := [MAX_LEVEL]*skipListNode{}
  	for i := sl.level - 1; i >= 0; i-- {
  		update[i] = sl.head
  		for nil != cur.forwards[i] {
  			if cur.forwards[i].score == score && cur.forwards[i].v == v {
  				update[i] = cur
  				break
  			}
  			cur = cur.forwards[i]
  		}
  	}
  
  	cur = update[0].forwards[0]
  	for i := cur.level - 1; i >= 0; i-- {
  		if update[i] == sl.head && cur.forwards[i] == nil {
  			sl.level = i
  		}
  
  		if nil == update[i].forwards[i] {
  			update[i].forwards[i] = nil
  		} else {
  			update[i].forwards[i] = update[i].forwards[i].forwards[i]
  		}
  	}
  
  	sl.length--
  
  	return 0
  }
  
  func (sl *SkipList) String() string {
  	return fmt.Sprintf("level:%+v, length:%+v", sl.level, sl.length)
  }
  
  ```



### 2.3 散列表

散列表的英文叫“Hash Table”，我们平时也叫它“哈希表”或者“Hash 表”

散列表用的是数组支持按照下标随机访问数据的特性，所以散列表其实就是数组的一种扩展，由数组演化而来。可以说，如果没有数组，就没有散列表。

我们可以截取参赛编号的后两位作为数组下标，来存取选手信息数据。当通过参赛编号查询选手信息的时候，我们用同样的方法，取参赛编号的后两位，作为数组下标，来读取数组中的数据。这就是典型的散列思想。其中，参赛选手的编号我们叫做键（key）或者关键字。我们用它来标识一个选手。我们把参赛编号转化为数组下标的映射方法就叫作散列函数（或“Hash 函数”“哈希函数”），而散列函数计算得到的值就叫作散列值（或“Hash 值”“哈希值”）。

<img src="https://static001.geekbang.org/resource/image/92/73/92c89a57e21f49d2f14f4424343a2773.jpg?wh=1142*744" alt="img" style="zoom:80%;" />

散列函数，顾名思义，它是一个函数。我们可以把它定义成 hash(key)，其中 key 表示元素的键值，hash(key) 的值表示经过散列函数计算得到的散列值。那第一个例子中，编号就是数组下标，所以 hash(key) 就等于 key。

要想找到一个不同的 key 对应的散列值都不一样的散列函数，几乎是不可能的。即便像业界著名的MD5、SHA、CRC等哈希算法，也无法完全避免这种**散列冲突**。而且，因为数组的存储空间有限，也会加大散列冲突的概率。

#### 散列冲突

##### 开放寻址法

开放寻址法的核心思想是，如果出现了散列冲突，我们就重新探测一个空闲位置，将其插入。那如何重新探测新的位置呢？我先讲一个比较简单的探测方法:

- #### 线性探测（Linear Probing）

  当我们往散列表中插入数据时，如果某个数据经过散列函数散列之后，存储位置已经被占用了，我们就从当前位置开始，依次往后查找，看是否有空闲位置，直到找到为止。我说的可能比较抽象，我举一个例子具体给你说明一下。这里面黄色的色块表示空闲位置，橙色的色块表示已经存储了数据。

​	从图中可以看出，散列表的大小为 10，在元素 x 插入散列表之前，已经 6 个元素插入到散列表中。x 经过 Hash 算法之后，被散列到位置下标为 7 的位置，但是这个位置已经有数据了，所以就产生了冲突。于是我们就顺序地往后一个一个找，看有没有空闲的位置，遍历到尾部都没有找到空闲的位置，于是我们再从表头开始找，直到找到空闲位置 2，于是将其插入到这个位置。

![img](https://static001.geekbang.org/resource/image/5c/d5/5c31a3127cbc00f0c63409bbe1fbd0d5.jpg?wh=1142*530)



在散列表中**查找元素**的过程有点儿类似插入过程。我们通过散列函数求出要查找元素的键值对应的散列值，然后比较数组中下标为散列值的元素和要查找的元素。如果相等，则说明就是我们要找的元素；否则就顺序往后依次查找。如果遍历到数组中的空闲位置，还没有找到，就说明要查找的元素并没有在散列表中。



散列表跟数组一样，不仅支持插入、查找操作，还支持**删除操作**。对于使用线性探测法解决冲突的散列表，删除操作稍微有些特别。我们**不能单纯地把要删除的元素设置为空**。这是为什么呢？还记得我们刚讲的查找操作吗？在查找的时候，一旦我们通过线性探测方法，找到一个空闲位置，我们就可以认定散列表中不存在这个数据。但是，如 果这个空闲位置是我们后来删除的，就会导致原来的查找算法失效。本来存在的数据，会被认定为不存在。这个问题如何解决呢？我们可以将删除的元素，特殊标记为 deleted。当线性探测查找的时候，遇到标记为 deleted 的空间，并不是停下来，而是继续往下探测。

**问题**

随着数据的不断增多，散列冲突发生的可能性就会越来越大，空闲位置会越来越少，线性探测的时间就会越来越久。极端情况下，我们可能需要探测整个散列表，所以最坏情况下的时间复杂度为 O(n)。同理，在删除和查找时，也有可能会线性探测整张散列表，才能找到要查找或者删除的数据。



- #### 二次探测（Quadratic probing）

  所谓二次探测，跟线性探测很像，线性探测每次探测的步长是 1，那它探测的下标序列就是 hash(key)+0，hash(key)+1，hash(key)+2……而二次探测探测的步长就变成了原来的“二次方”，也就是说，它探测的下标序列就是 hash(key)+0，hash(key)+1^2，hash(key)+2^2……

  

- #### 双重散列（Double hashing）

  双重散列，意思就是不仅要使用一个散列函数。我们使用一组散列函数 hash1(key)，hash2(key)，hash3(key)……我们先用第一个散列函数，如果计算得到的存储位置已经被占用，再用第二个散列函数，依次类推，直到找到空闲的存储位置

不管采用哪种探测方法，当散列表中空闲位置不多的时候，散列冲突的概率就会大大提高。为了尽可能保证散列表的操作效率，一般情况下，我们会尽可能保证散列表中有一定比例的空闲槽位。我们**用装载因子（load factor）来表示空位的多少。**

```
散列表的装载因子=填入表中的元素个数/散列表的长度
```

**装载因子越大，说明空闲位置越少，冲突越多，散列表的性能会下降。**



##### 链表法

链表法是一种更加常用的散列冲突解决办法，相比开放寻址法，它要简单很多。我们来看这个图，在散列表中，每个“桶（bucket）”或者“槽（slot）”会对应一条链表，所有散列值相同的元素我们都放到相同槽位对应的链表中。

<img src="https://static001.geekbang.org/resource/image/a4/7f/a4b77d593e4cb76acb2b0689294ec17f.jpg?wh=1142*640" alt="img" style="zoom:67%;" />

当插入的时候，我们只需要通过散列函数计算出对应的散列槽位，将其插入到对应链表中即可，所以插入的时间复杂度是 O(1)。当查找、删除一个元素时，我们同样通过散列函数计算出对应的槽，然后遍历链表查找或者删除。那查找或删除操作的时间复杂度是多少呢？实际上，这两个操作的时间复杂度跟链表的长度 k 成正比，也就是 O(k)。对于散列比较均匀的散列函数来说，理论上讲，k=n/m，其中 n 表示散列中数据的个数，m 表示散列表中“槽”的个数。



#### 散列表设计

如何设计一个可以应对各种异常情况的工业级散列表，来避免在散列冲突的情况下，散列表性能的急剧下降，并且能抵抗散列碰撞攻击？

**散列函数的设计不能太复杂**。过于复杂的散列函数，势必会消耗很多计算时间，也就间接地影响到散列表的性能。其次，**散列函数生成的值要尽可能随机并且均匀分布**，这样才能避免或者最小化散列冲突，而且即便出现冲突，散列到每个槽里的数据也会比较平均，不会出现某个槽内数据特别多的情况。实际工作中，我们还需要综合考虑各种因素。这些因素有关键字的长度、特点、分布、还有散列表的大小等。散列函数各式各样，举几个常用的、简单的散列函数的设计方法。

① **数据分析法**

学生运动会的例子，我们通过分析参赛编号的特征，把编号中的后两位作为散列值。我们还可以用类似的散列函数处理手机号码，因为手机号码前几位重复的可能性很大，但是后面几位就比较随机，我们可以取手机号的后四位作为散列值。



**② 使用hash函数**

第二个例子就是上一节的开篇思考题，如何实现 Word 拼写检查功能。这里面的散列函数，我们就可以这样设计：将单词中每个字母的ASCll 码值“进位”相加，然后再跟散列表的大小求余、取模，作为散列值。比如，英文单词 nice，我们转化出来的散列值就是下面这样：

```go
hash("nice")=(("n" - "a") * 26*26*26 + ("i" - "a")*26*26 + ("c" - "a")*26+ ("e"-"a")) / 78978
```

**如何避免低效的扩容？**

我们刚刚分析得到，大部分情况下，动态扩容的散列表插入一个数据都很快，但是在特殊情况下，当装载因子已经到达阈值，需要先进行扩容，再插入数据。这个时候，插入数据就会变得很慢，甚至会无法接受。

当有新数据要插入时，我们将新数据插入新散列表中，并且从老的散列表中拿出一个数据放入到新散列表。每次插入一个数据到散列表，我们都重复上面的过程。经过多次插入操作之后，老的散列表中的数据就一点一点全部搬移到新散列表中了。这样没有了集中的一次性数据搬移，插入操作就都变得很快了。

<img src="https://static001.geekbang.org/resource/image/6d/cb/6d6736f986ec4b75dabc5472965fb9cb.jpg?wh=1142*769" alt="img" style="zoom:67%;" />

##### **解决散列冲突的方法**

- #### 开放寻址法

  我们先来看看，开放寻址法的**优点**有哪些。

  开放寻址法不像链表法，需要拉很多链表。散列表中的数据都存储在数组中，可以有效地利用 CPU 缓存加快查询速度。而且，这种方法实现的散列表，序列化起来比较简单。链表法包含指针，序列化起来就没那么容易。你可不要小看序列化，很多场合都会用到的。我们后面就有一节会讲什么是数据结构序列化、如何序列化，以及为什么要序列化。

  我们再来看下，开放寻址法有哪些**缺点**。

  上一节我们讲到，用开放寻址法解决冲突的散列表，**删除数据的时候比较麻烦**，需要特殊标记已经删除掉的数据。而且，在开放寻址法中，所有的数据都存储在一个数组中，比起链表法来说，冲突的代价更高。所以，使用开放寻址法解决冲突的散列表，**装载因子的上限不能太大**。这也导致这种方法比链表法更浪费内存空间。

  **总结：当数据量比较小、装载因子小的时候，适合采用开放寻址法。**



- #### 链表法

  ​		链表法对内存的**利用率**比开放寻址法**要高**。因为链表结点可以在需要的时候再创建，并不需要像开放寻址法那样事先申请好。实际上，这一点也是我们前面讲过的链表优于数组的地方。**链表法**比起开放寻址法，**对大装载因子的容忍度更高**。开放寻址法只能适用装载因子小于 1 的情况。接近 1 时，就可能会有大量的散列冲突，导致大量的探测、再散列等，性能会下降很多。但是对于链表法来说，只要散列函数的值随机均匀，即便装载因子变成 10，也就是链表的长度变长了而已，虽然查找效率有所下降，但是比起顺序查找还是快很多。

  ​		**总结一下，基于链表的散列冲突处理方法比较适合存储大对象、大数据量的散列表，而且，比起开放寻址法，它更加灵活，支持更多的优化策略，比如用红黑树代替链表。**

  



## 3.哈希算法

**什么是哈希算法？**

哈希算法的定义和原理非常简单，基本上一句话就可以概括了。将任意长度的**二进制值串**映射为**固定长度的二进制值串**，这个**映射的规则**就是**哈希算法**，而通过原始数据映射之后得到的二进制值串就是哈希值。但是，要想设计一个优秀的哈希算法并不容易，

一个好的哈希算法：

- 从哈希值不能反向推导出原始数据（所以哈希算法也叫单向哈希算法）；
- 对输入数据非常敏感，哪怕原始数据只修改了一个 Bit，最后得到的哈希值也大不相同；
- 散列冲突的概率要很小，对于不同的原始数据，哈希值相同的概率非常小；
- 哈希算法的执行效率要尽量高效，针对较长的文本，也能快速地计算出哈希值。

### 哈希算法的主要应用

- #### 安全加密

  最常用于加密的哈希算法是 MD5（MD5 Message-Digest Algorithm，MD5 消息摘要算法）和 SHA（Secure Hash Algorithm，安全散列算法）。对用于加密的哈希算法来说，有两点格外重要。第一点是很难根据哈希值反向推导出原始数据，第二点是散列冲突的概率要很小。**一般情况下，哈希值越长的哈希算法，散列冲突的概率越低。**

  对于哈希算法被解密造成的“脱库”问题，**针对字典攻击，我们可以引入一个盐（salt）**，跟用户的密码组合在一起，增加密码的复杂度。我们拿组合之后的字符串来做哈希算法加密，将它存储到数据库中，进一步增加破解的难度。不过我这里想多说一句，我认为安全和攻击是一种博弈关系，不存在绝对的安全。所有的安全措施，只是增加攻击的成本而已。



- #### 唯一标识

  ​     我们知道，任何文件在计算中都可以表示成二进制码串，所以，比较笨的办法就是，拿要查找的图片的二进制码串与图库中所有图片的二进制码串一一比对。如果相同，则说明图片在图库中存在。但是，每个图片小则几十 KB、大则几 MB，转化成二进制是一个非常长的串，比对起来非常耗时。有没有比较快的方法呢？

  ​     我们可以从图片的二进制码串开头取 100 个字节，从中间取 100 个字节，从最后再取 100 个字节，然后将这 300 个字节放到一块，通过哈希算法（比如 MD5），得到一个哈希字符串，用它作为图片的唯一标识。通过这个唯一标识来判定图片是否在图库中，这样就可以减少很多工作量。



- #### 数据校验

  网络传输是不安全的，下载的文件块有可能是被宿主机器恶意修改过的，又或者下载过程中出现了错误，所以下载的文件块可能不是完整的。如果我们没有能力检测这种恶意修改或者文件下载出错，就会导致最终合并后的电影无法观看，甚至导致电脑中毒。现在的问题是，如何来校验文件块的安全、正确、完整呢？

  我们通过哈希算法，对 100 个文件块分别取哈希值，并且保存在种子文件中。我们在前面讲过，哈希算法有一个特点，对数据很敏感。只要文件块的内容有一丁点儿的改变，最后计算出的哈希值就会完全不同。所以，当文件块下载完成之后，我们可以通过相同的哈希算法，对下载好的文件块逐一求哈希值，然后跟种子文件中保存的哈希值比对。如果不同，说明这个文件块不完整或者被篡改了，需要再重新从其他宿主机器上下载这个文件块。应用四：散列函数

  

- #### 散列函数

  散列函数是设计一个散列表的关键。它直接决定了散列冲突的概率和散列表的性能。不过，相对哈希算法的其他应用，散列函数对于散列算法冲突的要求要低很多。即便出现个别散列冲突，只要不是过于严重，我们都可以通过开放寻址法或者链表法解决。

  

- #### 负载均衡

  负载均衡算法有很多，比如轮询、随机、加权轮询等。那如何才能实现一个会话粘滞（session sticky）的负载均衡算法呢？也就是说，我们需要在同一个客户端上，在一次会话中的所有请求都路由到同一个服务器上。

  **最直接的方法就是，维护一张映射关系表**，这张表的内容是**客户端 IP** 地址或者**会话 ID** 与服务器编号的映射关系。客户端发出的每次请求，都要先在映射表中查找应该路由到的服务器编号，然后再请求编号对应的服务器。这种方法简单直观，但也有几个弊端：

  - 如果客户端很多，映射表可能会很大，比较浪费内存空间；
  - 客户端下线、上线，服务器扩容、缩容都会导致映射失效，这样维护映射表的成本就会很大；

  如果借助哈希算法，这些问题都可以非常完美地解决。**我们可以通过哈希算法，对客户端 IP 地址或者会话 ID 计算哈希值，将取得的哈希值与服务器列表的大小进行取模运算，最终得到的值就是应该被路由到的服务器编号**。 这样，我们就可以把同一个 IP 过来的所有请求，都路由到同一个后端服务器上。

  

- #### 数据分片

  哈希算法还可以用于数据的分片。我这里有两个例子。

  - **如何统计“搜索关键词”出现的次数？**

    假如我们有 1T 的日志文件，这里面记录了用户的搜索关键词，我们想要快速统计出每个关键词被搜索的次数，该怎么做呢？我们来分析一下。这个问题有两个难点，第一个是搜索日志很大，没办法放到一台机器的内存中。第二个难点是，如果只用一台机器来处理这么巨大的数据，处理时间会很长。

    针对这两个难点，我们可以先对数据进行分片，然后采用多台机器处理的方法，来提高处理速度。具体的思路是这样的：为了提高处理的速度，**我们用 n 台机器并行处理**。我们从搜索记录的日志文件中，依次读出每个搜索关键词，并且**通过哈希函数计算哈希值，然后再跟 n 取模，最终得到的值，就是应该被分配到的机器编号。**

    **哈希值相同的搜索关键词就被分配到了同一个机器上。也就是说，同一个搜索关键词会被分配到同一个机器上。每个机器会分别计算关键词出现的次数，最后合并起来就是最终的结果。**

    

  - **如何快速判断图片是否在图库中？**

    假设现在我们的图库中有 1 亿张图片，很显然，在单台机器上构建散列表是行不通的。因为单台机器的内存有限，而 1 亿张图片构建散列表显然远远超过了单台机器的内存上限。

    我们同样**可以对数据进行分片**，然后采用多机处理。我们准备 n 台机器，让每台机器只维护某一部分图片对应的散列表。**我们每次从图库中读取一个图片，计算唯一标识，然后与机器个数 n 求余取模**，得到的值就对应要分配的机器编号，然后将这个图片的唯一标识和图片路径发往对应的机器构建散列表。当我们要判断一个图片是否在图库中的时候，我们通过同样的哈希算法，计算这个图片的唯一标识，然后与机器个数 n 求余取模。假设得到的值是 k，那就去编号 k 的机器构建的散列表中查找。

  

- #### **分布式存储**

  现在互联网面对的都是海量的数据、海量的用户。我们为了提高数据的读取、写入能力，一般都采用分布式的方式来存储数据，比如分布式缓存。我们有海量的数据需要缓存，所以一个缓存机器肯定是不够的。于是，我们就需要将数据分布在多台机器上。

  但是，如果数据增多，原来的 10 个机器已经无法承受了，我们就需要扩容了，**比如扩到 11 个机器**，**这时候麻烦就来了**。因为，这里并不是简单地加个机器就可以了。原来的数据是通过与 10 来取模的。比如 13 这个数据，存储在编号为 3 这台机器上。但是新加了一台机器中，我们对数据按照 11 取模，原来 13 这个数据就被分配到 2 号这台机器上了。

  这样，所有的数据都要重新计算哈希值，然后重新搬移到正确的机器上。这样就相当于，缓存中的数据一下子就都失效了。所有的数据请求都会穿透缓存，直接去请求数据库。**这样就可能发生雪崩效应，压垮数据库**。

  #### 这时候，一致性哈希算法就要登场了。



## 4.树

- #### 满二叉树

  叶子节点全都在最底层，除了叶子节点之外，每个节点都有左右两个子节点，这种二叉树就叫做**满二叉树**

- #### 完全二叉树

  叶子节点都在最底下两层，最后一层的叶子节点都靠左排列，并且除了最后一层，其他层的节点个数都要达到最大，这种二叉树叫做完全二叉树。

- #### 如何表示（或者存储）一棵二叉树？

  我们有两种方法，一种是基于指针或者引用的二叉链式存储法，一种是基于数组的顺序存储法。

  - **链式存储法**

    每个节点有三个字段，其中一个存储数据，另外两个是指向左右子节点的指针。我们只要拎住根节点，就可以通过左右子节点的指针，把整棵树都串起来。这种存储方式我们比较常用。大部分二叉树代码都是通过这种结构来实现的。

    ```go
    // 定义树节点
    type Node struct {
    	data  interface{}
    	left  *Node
    	right *Node
    }
    
    // 生成一个新的节点
    func NewNode(data interface{}) *Node {
    	return &Node{data: data}
    }
    
    // 定义一棵二叉树
    type BinaryTree struct {
    	root *Node
    }
    ```

  - **顺序存储法**

    顺序存储法。我们把根节点存储在下标 i = 1 的位置，那左子节点存储在下标 2 * i = 2 的位置，右子节点存储在 2 * i + 1 = 3 的位置。以此类推，B 节点的左子节点存储在 2 * i = 2 * 2 = 4 的位置，右子节点存储在 2 * i + 1 = 2 * 2 + 1 = 5 的位置。

    如果节点 X 存储在数组中下标为 i 的位置，下标为 2 * i 的位置存储的就是左子节点，下标为 2 * i + 1 的位置存储的就是右子节点。反过来，下标为 i/2 的位置存储就是它的父节点。通过这种方式，我们只要知道根节点存储的位置（一般情况下，为了方便计算子节点，根节点会存储在下标为 1 的位置），这样就可以通过下标计算，把整棵树都串起来。

    <img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\二叉树顺序存储.webp" style="zoom:80%;" />

如果某棵二叉树是一棵完全二叉树，那用数组存储无疑是最节省内存的一种方式。因为数组的存储方式并不需要像链式存储法那样，要存储额外的左右子节点的指针。这也是为什么完全二叉树会单独拎出来的原因，也是为什么完全二叉树要求最后一层的子节点都靠左的原因。

### 4.1二叉树的遍历

- #### 如何将所有节点都遍历打印出来呢？

  经典的方法有三种，**前序遍历、中序遍历和后序遍历**。其中，前、中、后序，表示的是节点与它的左右子树节点遍历打印的先后顺序。前序遍历是指，对于树中的任意节点来说，先打印这个节点，然后再打印它的左子树，最后打印它的右子树。中序遍历是指，对于树中的任意节点来说，先打印它的左子树，然后再打印它本身，最后打印它的右子树。后序遍历是指，对于树中的任意节点来说，先打印它的左子树，然后再打印它的右子树，最后打印这个节点本身。

  ```go
  package binarytree
  
  type TreeNode struct {
  	Val   int
  	Left  *TreeNode
  	Right *TreeNode
  }
  
  func preOrderTraversal(root *TreeNode) []int {
  	if root == nil {
  		return nil
  	}
  	if root.Left == nil && root.Right == nil {
  		return []int{root.Val}
  	}
  	var stack []*TreeNode
  	var res []int
  	stack = append(stack, root)
  	for len(stack) != 0 {
  		e := stack[len(stack)-1]
  		stack = stack[:len(stack)-1]
  		res = append(res, e.Val)
  		if e.Right != nil {
  			stack = append(stack, e.Right)
  		}
  		if e.Left != nil {
  			stack = append(stack, e.Left)
  		}
  	}
  	return res
  }
  
  func inOrderTraversal(root *TreeNode) []int {
  	if root == nil {
  		return nil
  	}
  	if root.Left == nil && root.Right == nil {
  		return []int{root.Val}
  	}
  	res := inOrderTraversal(root.Left)
  	res = append(res, root.Val)
  	res = append(res, inOrderTraversal(root.Right)...)
  
  	return res
  }
  
  func postOrderTraversal(root *TreeNode) []int {
  	if root == nil {
  		return nil
  	}
  	var res []int
  	if root.Left != nil {
  		lres := postOrderTraversal(root.Left)
  		if len(lres) > 0 {
  			res = append(res, lres...)
  		}
  	}
  	if root.Right != nil {
  		rres := postOrderTraversal(root.Right)
  		if len(rres) > 0 {
  			res = append(res, rres...)
  		}
  	}
  	res = append(res, root.Val)
  	return res
  }
  
  ```

  

### 4.2 二叉查找树

二叉查找树是二叉树中最常用的一种类型，也叫二叉搜索树。顾名思义，二叉查找树是为了实现快速查找而生的。不过，它不仅仅支持**快速查找一个数据，还支持快速插入、删除一个数据。**它是怎么做到这些的呢？

- ##### **二叉查找树要求，在树中的任意一个节点，其左子树中的每个节点的值，都要小于这个节点的值，而右子树节点的值都大于这个节点的值。**



#### 二叉查找树的查找操作

首先，我们看如何在二叉查找树中查找一个节点。我们先取根节点，如果它等于我们要查找的数据，那就返回。如果要查找的数据比根节点的值小，那就在左子树中递归查找；如果要查找的数据比根节点的值大，那就在右子树中递归查找。

- **二叉查找树的定义**

  ```go
  type BST struct {
  	*BinaryTree
  	//比对函数，0:v==nodeV,正数:v>nodeV,负数:v<nodeV
  	compareFunc func(v, NodeV interface{}) int
  }
  ```

- **新建二叉查找树**

  ```go
  func NewBST(rootV interface{}, compareFunc func(v, nodeV interface{}) int) *BST {
  	if nil == compareFunc {
  		return nil
  	}
  	return &BST{BinaryTree: NewBinaryTree(rootV), compareFunc: compareFunc}
  }
  ```

- **二叉查找树的查找操作**

  ```go
  func (b *BST) Find(v interface{}) *TreeNode {
  	p := b.root
  	for p != nil {
  		cr := b.compareFunc(v, p.Val)
  		if cr == 0 {
  			return p
  		} else if cr > 0 {
  			p = p.Right
  		} else {
  			p = p.Left
  		}
  	}
  	return nil
  }
  ```






#### 二叉查找树的插入操作

​		如果要插入的数据比节点的数据大，并且节点的右子树为空，就将新数据直接插到右子节点的位置；如果不为空，就再递归遍历右子树，查找插入位置。同理，如果要插入的数据比节点数值小，并且节点的左子树为空，就将新数据插入到左子节点的位置；如果不为空，就再递归遍历左子树，查找插入位置。

```go
// Insert 插入操作
func (b *BST) Insert(v interface{}) bool {
	p := b.root
	for p != nil {
		cr := b.compareFunc(v, p.Val)
		if cr == 0 {
			return false
		} else if cr > 0 {
			if p.Right == nil {
				p.Right = NewNode(v)
				break
			}
			p = p.Right
		} else {
			if p.Left == nil {
				p.Left = NewNode(v)
				break
			}
			p = p.Left
		}
	}
	return true
}
```



#### 二叉查找树的删除操作

二叉查找树的查找、插入操作都比较简单易懂，但是它的删除操作就比较复杂了 。

针对要删除节点的子节点个数的不同，**我们需要分三种情况来处理。**

- 第一种情况是，如果要**删除的节点没有子节点**，我们只需要直接将父节点中，指向要删除节点的指针置为 null。比如图中的删除节点 55。

- 第二种情况是，如果**要删除的节点只有一个子节点**（只有左子节点或者右子节点），我们只需要更新父节点中，指向要删除节点的指针，让它指向要删除节点的子节点就可以了。比如图中的删除节点 13。

- 第三种情况是，**如果要删除的节点有两个子节点**，这就比较复杂了。**我们需要找到这个节点的右子树中的最小节点**，把它替换到要删除的节点上。然后再删除掉这个最小节点，因为最小节点肯定没有左子节点（如果有左子结点，那就不是最小节点了），所以，我们可以应用上面两条规则来删除这个最小节点。

  ![](C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\二叉查找树搜索.webp)

```go
// Delete 删除操作
func (b *BST) Delete(v interface{}) bool {
	var pp *TreeNode = nil
	p := b.root
	deleteLeft := false
	for p != nil {
		cr := b.compareFunc(v, p.Val)
		if cr > 0 {
			pp = p
			deleteLeft = false
			p = p.Right
		} else if cr < 0 {
			pp = p
			deleteLeft = true
			p = p.Left
		} else {
			// 如果找到了 退出循环 或者 p为nil => 没找到
			break
		}
	}
	if p == nil {
		return false
	} else if p.Left == nil && p.Right == nil {
		// 被删除的节点是叶子节点 pp是他的父节点
		if pp != nil {
			if deleteLeft {
				// 要删除的节点是左节点
				pp.Left = nil
			} else {
				pp.Right = nil
			}
		} else {
			// 要删除的是跟节点
			b.root = nil
		}
	} else if p.Right != nil { //删除的是一个有右孩子，不一定有左孩子的节点
		// 找到p节点右孩子的最小节点
		pq := p
		q := p.Right
		fromRight := true
		for nil != q.Left { //向左走到底
			pq = q
			q = q.Left
			fromRight = false
		}
		if fromRight {
			pq.Right = nil
		} else {
			pq.Left = nil
		}
		q.Left = p.Left
		q.Right = p.Right
		if pp == nil {
			//	根节点被删除
			b.root = q
		} else {
			if deleteLeft {
				pp.Left = q
			} else {
				pp.Right = q
			}
		}
	} else {
		//	删除的是只有一个左孩子节点
		if pp != nil {
			if deleteLeft {
				pp.Left = p.Left
			} else {
				pp.Right = p.Right
			}
		} else {
			b.root = p.Left
		}
	}
	return true
}

```



#### 支持重复数据的二叉查找树

很多时候，在实际的软件开发中，我们在二叉查找树中存储的，是一个包含很多字段的对象。**我们利用对象的某个字段作为键值（key）来构建二叉查找树**。我们把对象中的其他字段叫作卫星数据。

如果存储的两个对象键值相同，这种情况该怎么处理呢？我这里有两种解决方法。

- 第一种方法比较容易。二叉查找树中**每一个节点不仅会存储一个数据**，因此我们通过链表和支持动态扩容的数组等数据结构，**把值相同的数据都存储在同一个节点上**。

- 第二种方法比较不好理解，不过更加优雅。每个节点仍然只存储一个数据。在查找插入位置的过程中，如果碰到一个节点的值，与要插入数据的值相同，我们就将这个要插入的数据放到这个节点的右子树，也就是说，**把这个新插入的数据当作大于这个节点的值来处理**。

  - 当要**查找数据**的时候，遇到值相同的节点，我们并不停止查找操作，而是继续在右子树中查找，直到遇到**叶子节点**，才停止。**这样就可以把键值等于要查找值的所有节点都找出来。**
  - 对于删除操作，**我们也需要先查找到每个要删除的节点**，然后再按前面讲的删除操作的方法，依次删除。




#### 平衡二叉查找树

平衡二叉树的严格定义是这样的：**二叉树中任意一个节点的左右子树的高度相差不能大于 1**。从这个定义来看，上一节我们讲的完全二叉树、满二叉树其实都是平衡二叉树，但是非完全二叉树也有可能是平衡二叉树。

![](C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\平衡二叉树.webp)

**平衡二叉查找树**不仅满足上面**平衡二叉树的定义**，**还满足二叉查找树的特点**。最先被发明的平衡二叉查找树是AVL 树，它严格符合我刚讲到的平衡二叉查找树的定义，即任何节点的左右子树高度相差不超过 1，是一种高度平衡的二叉查找树。

- #### 如何定义一棵“红黑树”？

  顾名思义，红黑树中的节点，一类被标记为黑色，一类被标记为红色。除此之外，一棵红黑树还需要满足这样几个要求：

  - **根节点是黑色的**；
  - **每个叶子节点都是黑色的空节点（NIL）**，也就是说，**叶子节点不存储数据**；
  - 任何相邻的节点都不能同时为红色，也就是说，**红色节点是被黑色节点隔开的；**
  - **每个节点，从该节点到达其可达叶子节点的所有路径，都包含相同数目的黑色节点**；

![](C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\红黑树.webp)

- ### 为什么要使用红黑树而不是其他的平衡二叉查找树？

  ​		AVL 树是一种高度平衡的二叉树，所以查找的效率非常高，但是，有利就有弊，**AVL 树为了维持这种高度的平衡，就要付出更多的代价。每次插入、删除都要做调整，就比较复杂、耗时**。所以，对于有频繁的插入、删除操作的数据集合，使用 AVL 树的代价就有点高了。**红黑树只是做到了近似平衡，并不是严格的平衡，所以在维护平衡的成本上，要比 AVL 树要低。**所以，**红黑树的插入、删除、查找各种操作性能都比较稳定。对于工程应用来说，要面对各种异常情况，为了支撑这种工业级的应用，我们更倾向于这种性能稳定的平衡二叉查找树。**



左旋（rotate left）、右旋（rotate right）。**左旋全称其实是叫围绕某个节点的左旋**，那右旋的全称估计你已经猜到了，就叫围绕某个节点的右旋。我们下面的平衡调整中，会一直用到这两个操作，所以我这里画了个示意图，帮助你彻底理解这两个操作。图中的 a，b，r 表示子树，可以为空。

![](C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\左旋优选webp.webp)

- #### 插入操作的平衡调整

  红黑树规定，**插入的节点必须是红色的**。而且，**二叉查找树中新插入的节点都是放在叶子节点上**。所以，关于插入操作的平衡调整，有这样两种特殊情况，但是也都非常好处理。

  - 如果插入节点的父节点是黑色的，那我们什么都不用做，它仍然满足红黑树的定义。
  - 如果插入的节点是根节点，那我们直接改变它的颜色，把它变成黑色就可以了。

  **红黑树的平衡调整过程是一个迭代的过程。我们把正在处理的节点叫做关注节点。关注节点会随着不停地迭代处理，而不断发生变化。最开始的关注节点就是新插入的节点**

  

  - **CASE 1：如果关注节点是 a，它的叔叔节点 d 是红色，我们就依次执行下面的操作：**

    - **将关注节点 a 的父节点 b、叔叔节点 d 的颜色都设置成黑色；**
    - **将关注节点 a 的祖父节点 c 的颜色设置成红色；**
    - **关注节点变成 a 的祖父节点 c；**
    - **跳到 CASE 2 或者 CASE 3。**

    ![](C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\case1.webp)

    

    

  - **CASE 2：如果关注节点是 a，它的叔叔节点 d 是黑色，关注节点 a 是其父节点 b 的右子节点，我们就依次执行下面的操作：**

    - 关注节点变成节点 a 的父节点 b；
    - 围绕新的关注节点b 左旋；
    - 跳到 CASE 3

    ![](C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\case2.webp)

    

  - CASE 3：如果关注节点是 a，它的叔叔节点 d 是黑色，关注节点 a 是其父节点 b 的左子节点，我们就依次执行下面的操作：

    - 围绕关注节点 a 的祖父节点 c 右旋；

    - 将关注节点 a 的父节点 b、兄弟节点 c 的颜色互换。

    - 调整结束。

      ![](C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\case3。webp.webp)





### 4.3 递归树

递归的思想就是，**将大问题分解为小问题来求解，然后再将小问题分解为小小问题。这样一层一层地分解，直到问题的数据规模被分解得足够小，不用继续递归分解为止**。

#### 分析快速排序的时间复杂度

```go
func quciksort(arr []int,left,right int){
	if left < right{
		loc := partition(arr,left,right)
		quicksort(arr,left,loc-1)
		quicksort(arr,loc+1,right)
	}
}

func partition(arr []int,left,right int) int {
	i := left + 1
	j := right
	for i<j{
		if arr[i] > arr[left]{
			arr[i] ,arr[j] = arr[j],arr[i]
			j--
		}else{
			i++
		}
	}
	if arr[i] >= arr[left]{
		i--
	}
	arr[i] ,arr[left] = arr[left],arr[i]
}
```

- #### 使用递归树进行分析

  <img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\快速排序递归树.webp" style="zoom:80%;" />

快速排序的过程中，**每次分区都要遍历待分区区间的所有数据**，所以，**每一层分区操作所遍历的数据的个数之和就是 n**。我们现在只要求出**递归树的高度 h**，这个快排过程遍历的数据个数就是 **h∗n** ，也就是说，时间复杂度就是 O**(h∗n)**。

我们知道，快速排序结束的条件就是待排序的小区间，大小为 1，也就是说叶子节点里的数据规模是 1。从根节点 n 到叶子节点 1，递归树中最短的一个路径每次都乘以(1/10)，最长的一个路径每次都乘以(9/10)。通过计算，我们可以得到，从根节点到叶子节点的最短路径是 ，最长的路径是 log(10/9)n。

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\最长路径.webp" style="zoom:80%;" />

所以，遍历数据的个数总和就介于 `nlog(10)n` 和 `nlog(10/9)n` 之间。根据复杂度的大 O 表示法，对数复杂度的底数不管是多少，我们统一写成 `logn`，所以，当分区大小比例是 1:9 时，快速排序的时间复杂度仍然是 O(`nlogn`)。

也就是说，对于 k 等于 9，99，甚至是 999，9999……，只要 k 的值不随 n 变化，是一个事先确定的常量，那快排的时间复杂度就是 O(`nlogn`)。所以，从概率论的角度来说，快排的平均时间复杂度就是 O(`nlogn`)。



#### 分析斐波那契数列的时间复杂度

```go
int f(int n) {
  if (n == 1) return 1;
  if (n == 2) return 2;
  return f(n-1) + f(n-2);
}
```

这样一段代码的时间复杂度是多少呢？你可以先试着分析一下，然后再来看，我是怎么利用递归树来分析的。我们先把上面的递归代码画成递归树，就是下面这个样子：

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\fibna.webp" style="zoom:80%;" />

f(n) 分解为 f(n−1) 和 f(n−2)，每次数据规模都是 −1 或者 −2，叶子节点的数据规模是 1 或者 2。所以，从根节点走到叶子节点，每条路径是长短不一的。如果每次都是 −1，那最长路径大约就是 n；如果每次都是 −2，那最短路径大约就是 n/2

每次分解之后的合并操作只需要一次加法运算，我们把这次加法运算的时间消耗记作 1。所以，从上往下，第一层的总时间消耗是 1，第二层的总时间消耗是 2，第三层的总时间消耗就是 2^2。依次类推，第 k 层的时间消耗就是 2^k−1，那整个算法的总的时间消耗就是每一层时间消耗之和。

**如果路径长度都为 n，那这个总和就是 2^n−1**。

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\1.webp" style="zoom:80%;" />

**如果路径长度都是 `n/2` ，那整个算法的总的时间消耗就是 2^(n/2)−1。**

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\2.webp" style="zoom:80%;" />

所以，这个算法的时间复杂度就介于 O(2^n) 和 O(2^(n/2)) 之间。虽然这样得到的结果还不够精确，只是一个范围，但是我们也基本上知道了上面算法的时间复杂度是指数级的，非常高。



#### 分析全排列的时间复杂度

前面两个复杂度分析都比较简单，我们再来看个稍微复杂的。

我们在高中的时候都学过排列组合。“如何把 n 个数据的所有排列都找出来”，这就是全排列的问题。我来举个例子。比如，1，2，3 这样 3 个数据，有下面这几种不同的排列：

```
1, 2, 3
1, 3, 2
2, 1, 3
2, 3, 1
3, 1, 2
3, 2, 1
```

如何用编程来进行实现呢？

如果我们确定了最后一位数据，那就变成了求解剩下 n−1 个数据的排列问题。而最后一位数据可以是 n 个数据中的任意一个，因此它的取值就有 n 种情况。所以，**“n 个数据的排列”问题，就可以分解成 n 个“n−1 个数据的排列”的子问题**。

**递推公式**

```
假设数组中存储的是1，2， 3...n。
        
f(1,2,...n) = {最后一位是1, f(n-1)} + {最后一位是2, f(n-1)} +...+{最后一位是n, f(n-1)}。
```

**代码**

```go
// PrintAll k代表需要处理的子数组的长度
func PrintAll(a []int, x int, k int) {
	if k == 1 {
		for i := 0; i < x; i++ {
			fmt.Print(a[i], " ")
		}
		fmt.Println()
	}
	for i := 0; i < k; i++ {
		a[i], a[k-1] = a[k-1], a[i]

		PrintAll(a, x, k-1)
		a[i], a[k-1] = a[k-1], a[i]
	}
}
```

如果不用我前面讲的递归树分析方法，这个递归代码的时间复杂度会比较难分析。现在，我们来看下，如何借助递归树，轻松分析出这个代码的时间复杂度。首先，我们还是画出递归树。不过，现在的递归树已经不是标准的二叉树了。

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\递归树.webp" style="zoom:80%;" />

第一层分解有 n 次交换操作，第二层有 n 个节点，每个节点分解需要 n−1 次交换，所以第二层总的交换次数是 n∗(n−1)。第三层有 n∗(n−1) 个节点，每个节点分解需要 n−2 次交换，所以第三层总的交换次数是 n∗(n−1)∗(n−2)。

以此类推，第 k 层总的交换次数就是 n∗(n−1)∗(n−2)∗...∗(n−k+1)。最后一层的交换次数就是 n∗(n−1)∗(n−2)∗...∗2∗1。每一层的交换次数之和就是总的交换次数。

```
n + n*(n-1) + n*(n-1)*(n-2) +... + n*(n-1)*(n-2)*...*2*1
```

这个公式的求和比较复杂，我们看最后一个数，n∗(n−1)∗(n−2)∗...∗2∗1 等于 n!，而前面的 n−1 个数都小于最后一个数，所以，总和肯定小于 n∗n!，也就是说，全排列的递归算法的时间复杂度大于 O(n!)，小于 O(n∗n!)，虽然我们没法知道非常精确的时间复杂度，但是这样一个范围已经让我们知道，全排列的时间复杂度是非常高的。



## 5.如何理解“堆”？

前面我们提到，堆是一种特殊的树。我们现在就来看看，什么样的树才是堆。我罗列了两点要求，只要满足这两点，它就是一个堆。

- **堆是一个完全二叉树；**
- **堆中每一个节点的值都必须大于等于（或小于等于）其子树中每个节点的值。**

> **第一点，堆必须是一个完全二叉树。还记得我们之前讲的完全二叉树的定义吗？完全二叉树要求，除了最后一层，其他层的节点个数都是满的，最后一层的节点都靠左排列。**
>
> **第二点，堆中的每个节点的值必须大于等于（或者小于等于）其子树中每个节点的值。实际上，我们还可以换一种说法，堆中每个节点的值都大于等于（或者小于等于）其左右子节点的值。这两种表述是等价的。对于每个节点的值都大于等于子树中每个节点值的堆，我们叫做“大顶堆”。对于每个节点的值都小于等于子树中每个节点值的堆，我们叫做“小顶堆”**

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\堆.webp" style="zoom:67%;" />

其中第 1 个和第 2 个是大顶堆，第 3 个是小顶堆，第 4 个不是堆。除此之外，从图中还可以看出来，对于同一组数据，我们可以构建多种不同形态的堆。

### 如何实现一个堆？

​		完全二叉树比较适合用数组来存储。用数组来存储完全二叉树是非常节省存储空间的。**因为我们不需要存储左右子节点的指针**，**单纯地通过数组的下标，就可以找到一个节点的左右子节点和父节点。**

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\存储堆.webp" style="zoom:67%;" />

数组中下标为 i 的节点的左子节点，就是下标为 i∗2 的节点，右子节点就是下标为 i∗2+1 的节点，父节点就是下标为 2i 的节点。 



### 向堆中插入一个元素

插入一个元素，进行调整，让其重新满足堆的特性，这个过程我们起了一个名字，就叫做**堆化（heapify）**。

堆化实际上有两种，从下往上和从上往下。这里我先讲**从下往上**的堆化方法。

堆化非常简单，就是**顺着节点所在的路径，向上或者向下，对比，然后交换。**

我们可以让新插入的节点与父节点对比大小。如果不满足子节点小于等于父节点的大小关系，我们就互换两个节点。一直重复这个过程，直到父子节点之间满足刚说的那种大小关系。

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\对话对话.webp" style="zoom:67%;" />

```go
type Heap struct{
	type Heap struct {
	// 一个数组切片 用于存储数据
	arr []int
	// 堆数组的容量
	cap int
	// 当前堆中的元素个数
	count int
}

// NewHeap  Init a new Heap
func NewHeap(cap int) *Heap {
	heap := &Heap{}
	heap.cap = cap
	heap.arr = make([]int, cap+1)
	heap.count = 0
	return heap
}

// top => Max  From down to up
func (h *Heap) insert(data int) {
	// 判断堆满
	if h.count == h.cap {
		return
	}
	h.count++
	h.arr[h.count] = data

	//compare with parent node
	i := h.count
	parent := i / 2
	for parent > 0 && h.arr[parent] < h.arr[i] {
		h.arr[i], h.arr[parent] = h.arr[parent], h.arr[parent]
		i = parent
		parent = i / 2
	}
}
}
```



### 删除堆顶元素

任何节点的值都大于等于（或小于等于）子树节点的值，我们可以发现，堆顶元素存储的就是堆中数据的最大值或者最小值。

假设我们构造的是大顶堆，堆顶元素就是最大的元素。当我们删除堆顶元素之后，就需要把第二大的元素放到堆顶，那第二大元素肯定会出现在左右子节点中。然后我们再迭代地删除第二大节点，以此类推，直到叶子节点被删除。

我们把最后一个节点放到堆顶，然后利用同样的父子节点对比方法。对于不满足父子节点大小关系的，互换两个节点，并且重复进行这个过程，直到父子节点之间满足大小关系为止。这就是从上往下的堆化方法。

因为我们移除的是数组中的最后一个元素，而在堆化的过程中，都是交换操作，不会出现数组中的“空洞”，所以这种方法堆化之后的结果，肯定满足完全二叉树的特性。

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\刪除堆顶元素.webp" style="zoom:67%;" />

```go
func (h *Heap) RemoveMax() {
	if h.count == 0 {
		return
	}
	h.arr[1], h.arr[h.count] = h.arr[h.count], h.arr[1]
	h.count--
	HeaptifyUpToDown(h.arr, h.count)
}


func HeaptifyUpToDown(a []int, n int) {
	for i := 1; i <= n/2; {
		maxIndex := i
		if a[i] < a[i*2] {
			maxIndex = i * 2
		}
		if i*2+1 <= n && a[maxIndex] < a[i*2+1] {
			maxIndex = i*2 + 1
		}
		if maxIndex == i {
			break
		}
		a[i], a[maxIndex] = a[maxIndex], a[i]
		i = maxIndex
	}
}
```

我们知道，一个包含 n 个节点的完全二叉树，树的高度不会超过 log2n。堆化的过程是顺着节点所在路径比较交换的，所以堆化的时间复杂度跟树的高度成正比，也就是 O(logn)。插入数据和删除堆顶元素的主要逻辑就是堆化，所以，往堆中插入一个元素和删除堆顶元素的时间复杂度都是 O(logN)。



### 基于堆实现排序

​		堆排序的时间复杂度非常稳定，是 **O(nlogn)，并且它还是原地排序算法**。如此优秀，它是怎么做到的呢？我们可以把堆排序的过程大致分解成两个大的步骤，**建堆和排序**。

- #### 建堆

  建堆的过程，有两种思路

  - 第一种是借助我们前面讲的，在堆中插入一个元素的思路。尽管数组中包含 n 个数据，但是我们可以假设，起初堆中只包含一个数据，就是下标为 1 的数据。然后，我们调用前面讲的插入操作，将下标从 2 到 n 的数据依次插入到堆中。这样我们就将包含 n 个数据的数组，组织成了堆。
  - 第二种实现思路，跟第一种截然相反，也是我这里要详细讲的。第一种建堆思路的处理过程是从前往后处理数组数据，并且每个数据插入堆中时，都是从下往上堆化。而第二种实现思路，是**从后往前处理数组，并且每个数据都是从上往下堆化**。

因为叶子节点往下堆化只能自己跟自己比较，所以我们直接从最后一个非叶子节点开始，依次堆化就行了。

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\11.webp" style="zoom:67%;" />

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\22.webp" style="zoom:67%;" />

```go
// BuildHeap 初始化一个堆
func BuildHeap(a []int, n int) {
	//heaptify from the last parent node
	for i := n / 2; i >= 1; i-- {
		heaptifyDownToLow(a, i, n)
	}
}

func heaptifyDownToLow(a []int, top, count int) {
	for i := top; i <= count/2; {
		maxIndex := i
		if a[i] < a[i*2] {
			maxIndex = i * 2
		}
		if i*2+1 <= count && a[i*2] < a[i*2+1] {
			maxIndex = i*2 + 1
		}
		if i == maxIndex {
			break
		}
		a[i], a[maxIndex] = a[maxIndex], a[i]
		i = maxIndex
	}
}
```

- #### 建堆操作的时间复杂度

  每个节点堆化的时间复杂度是 **`O(logn)`**，那 2n+1 个节点堆化的总时间复杂度是不是就是 **`O(nlogn)`** 呢？这个答案虽然也没错，但是这个值还是不够精确。实际上，堆排序的建堆过程的时间复杂度是 **`O(n)`**。

  <img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\节点个数.webp" style="zoom:67%;" />

  我们将每个非叶子节点的高度求和，就是下面这个公式：这个公式的求解稍微有点技巧，不过我们高中应该都学过：把公式左右都乘以 2，就得到另一个公式 S2。我们将 S2 错位对齐，并且用 S2 减去 S1，可以得到 S

  <img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\公式.webp" style="zoom:67%;" />

**S 的中间部分是一个等比数列**，所以最后可以用等比数列的求和公式来计算，最终的结果就是下面图中画的这个样子。

因为 h=log2n，代入公式 S，就能得到 S=O(n)，所以，建堆的时间复杂度就是 O(n)。



- #### 排序

  建堆结束之后，数组中的数据已经是按照大顶堆的特性来组织的。数组中的第一个元素就是堆顶，也就是最大的元素。我们把它跟最后一个元素交换，那最大元素就放到了下标为 n 的位置。

  这个过程有点类似上面讲的“**删除堆顶元素**”的操作，当堆顶元素移除之后，我们把下标为 n 的元素放到堆顶，然后再通过堆化的方法，将剩下的 n−1 个元素重新构建成堆。堆化完成之后，我们再取堆顶的元素，放到下标是 n−1 的位置，一直重复这个过程，直到最后堆中只剩下标为 1 的一个元素，排序工作就完成了。

  ```go
  func HeapSort(a []int, n int) {
  	BuildHeap(a, n)
  	k := n
  	for k >= 1 {
  		a[k], a[1] = a[1], a[k]
  		heaptifyUpToLow(a, 1, k-1)
  		k--
  	}
  }
  ```

  现在，我们再来分析一下堆排序的时间复杂度、空间复杂度以及稳定性。整个堆排序的过程，都只需要极个别临时存储空间，所以堆排序是原地排序算法。堆排序包括建堆和排序两个操作，建堆过程的时间复杂度是 O(n)，排序过程的时间复杂度是 **`O(nlogn)`**，所以，堆排序整体的时间复杂度是 **`O(nlogn)`**。



- #### 为什么数组下标从1开始？

  那如果从 0 开始存储，实际上处理思路是没有任何变化的，唯一变化的，可能就是，代码实现的时候，计算子节点和父节点的下标的公式改变了。如果节点的下标是 i，那左子节点的下标就是 2∗i+1，右子节点的下标就是 2∗i+2，父节点的下标就是 2i−1。

- #### 在实际开发中，为什么快速排序要比堆排序性能好？

  **第一点，堆排序数据访问的方式没有快速排序友好。**对于快速排序来说，数据是顺序访问的。而对于堆排序来说，数据是跳着访问的。 比如，堆排序中，最重要的一个操作就是数据的堆化。比如下面这个例子，对堆顶节点进行堆化，会依次访问数组下标是 1，2，4，8 的元素，而不是像快速排序那样，局部顺序访问，所以，这样对 CPU 缓存是不友好的。

  第二点，**对于同样的数据，在排序过程中，堆排序算法的数据交换次数要多于快速排序**。我们在讲排序的时候，提过两个概念，有序度和逆序度。对于基于比较的排序算法来说，整个排序过程就是由两个基本的操作组成的，比较和交换（或移动）。快速排序数据交换的次数不会比逆序度多。但是堆排序的第一步是建堆，建堆的过程会打乱数据原有的相对先后顺序，导致原数据的有序度降低。比如，对于一组已经有序的数据来说，经过建堆之后，数据反而变得更无序了。



### 堆的应用

- ### 优先级队列

  优先级队列，顾名思义，它首先应该是一个队列。我们前面讲过，队列最大的特性就是先进先出。不过，**在优先级队列中，**数据的出队顺序不是先进先出，而是**按照优先级来，优先级最高的，最先出队**。

  往优先级队列中插入一个元素，就相当于往堆中插入一个元素；从优先级队列中取出优先级最高的元素，就相当于取出堆顶元素。

  #### 1、合并有序小文件

  假设我们有 100 个小文件，每个文件的大小是 100MB，每个文件中存储的都是有序的字符串。我们希望将这些 100 个小文件合并成一个有序的大文件。这里就会用到优先级队列。

  我们从这 100 个文件中，**各取第一个字符串，放入数组中**，**然后比较大小，把最小的那个字符串放入合并后的大文件中**，**并从数组中删除**。

  假设，这个最小的字符串来自于 13.txt 这个小文件，我们就再从这个小文件取下一个字符串，放到数组中，重新比较大小，并且选择最小的放入合并后的大文件，将它从数组中删除。依次类推，直到所有的文件中的数据都放入到大文件为止。

  这里我们用数组这种数据结构，来存储从小文件中取出来的字符串。每次从数组中取最小字符串，**都需要循环遍历整个数组，显然，这不是很高效。有没有更加高效方法呢？**

  ​		这里就可以用到**优先级队列**，**也可以说是堆**。我们将从**小文件中取出来的字符串放入到小顶堆中，那堆顶的元素，也就是优先级队列队首的元素，就是最小的字符串**。我们将这个字符串放入到大文件中，并将其从堆中删除。然后再从小文件中取出下一个字符串，放入到堆中。循环这个过程，就可以将 100 个小文件中的数据依次放入到大文件中。**我们知道，删除堆顶数据和往堆中插入数据的时间复杂度都是 O(logn)，n 表示堆中的数据个数，这里就是 100**。是不是比原来数组存储的方式高效了很多呢？

  

  #### 2、高性能定时器

  ​		假设我们有一个定时器，定时器中维护了很多定时任务，每个任务都设定了一个要触发执行的时间点。定时器每过一个很小的单位时间（比如 1 秒），就扫描一遍任务，看是否有任务到达设定的执行时间。如果到达了，就拿出来执行。

  <img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\计时器.webp" style="zoom:67%;" />

  但是，这样每过 1 秒就扫描一遍任务列表的做法比较低效，主要原因有两点：第一，任务的约定执行时间离当前时间可能还有很久，这样前面很多次扫描其实都是徒劳的；第二，每次都要扫描整个任务列表，如果任务列表很大的话，势必会比较耗时。

  我们就可以用优先级队列来解决。我们按照任务设定的执行时间，将这些任务存储在优先级队列中，队列首部（也就是小顶堆的堆顶）存储的是最先执行的任务。

  这样，定时器就不需要每隔 1 秒就扫描一遍任务列表了。它拿队首任务的执行时间点，与当前时间点相减，得到一个时间间隔 T。这个时间间隔 T 就是，**从当前时间开始，需要等待多久，才会有第一个任务需要被执行。这样，定时器就可以设定在 T 秒之后，再来执行任务。从当前时间点到（T-1）秒这段时间里，定时器都不需要做任何事情。**当 T 秒时间过去之后，定时器取优先级队列中队首的任务执行。然后再计算新的队首任务的执行时间点与当前时间点的差值，把这个值作为定时器执行下一个任务需要等待的时间。这样，定时器既不用间隔 1 秒就轮询一次，也不用遍历整个任务列表，性能也就提高了。

  

- ### 利用堆求 Top K

  ​		我把这种求 Top K 的问题抽象成两类。一类是针对静态数据集合，也就是说数据集合事先确定，不会再变。另一类是针对动态数据集合，也就是说数据集合事先并不确定，有数据动态地加入到集合中。

  ​		针对静态数据，如何在一个包含 n 个数据的数组中，查找前 K 大数据呢？**我们可以维护一个大小为 K 的小顶堆，顺序遍历数组，从数组中取出数据与堆顶元素比较。如果比堆顶元素大，我们就把堆顶元素删除，并且将这个元素插入到堆中；如果比堆顶元素小，则不做处理，继续遍历数组。这样等数组中的数据都遍历完之后，堆中的数据就是前 K 大数据了。**

  ​		遍历数组需要 O(n) 的时间复杂度，一次堆化操作需要 O(logK) 的时间复杂度，所以最坏情况下，n 个元素都入堆一次，时间复杂度就是 O(nlogK)。

  针对动态数据求得 Top K 就是实时 Top K。怎么理解呢？我举一个例子。**一个数据集合中有两个操作，一个是添加数据，另一个询问当前的前 K 大数据。**

  ​		如果每次询问前 K 大数据，我们都基于当前的数据重新计算的话，那时间复杂度就是 O(nlogK)，n 表示当前的数据的大小。实际上，我们可以一直都维护一个 K 大小的小顶堆，当有数据被添加到集合中时，我们就拿它与堆顶的元素对比。如果比堆顶元素大，我们就把堆顶元素删除，并且将这个元素插入到堆中；如果比堆顶元素小，则不做处理。这样，无论任何时候需要查询当前的前 K 大数据，我们都可以立刻返回给他。

  

- ### 利用堆求中位数

  前面我们讲了如何求 Top K 的问题，现在我们来讲下，如何求动态数据集合中的中位数。中位数，顾名思义，就是处在中间位置的那个数。如果数据的个数是奇数，把数据从小到大排列，那第 (n/2)+1 个数据就是中位数（注意：假设数据是从 0 开始编号的）；如果数据的个数是偶数的话，那处于中间位置的数据有两个，第 n/2 个和第 (n/2 )+1个数据，这个时候，我们可以随意取一个作为中位数，比如取两个数中靠前的那个，就是第 n/2 个数据。

  <img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\中位数.webp" style="zoom:67%;" />

  对于一组静态数据，中位数是固定的，**我们可以先排序，第 n/2 个数据就是中位数。每次询问中位数的时候，我们直接返回这个固定的值就好了**。所以，尽管排序的代价比较大，但是边际成本会很小。但是，**如果我们面对的是动态数据集合，中位数在不停地变动，如果再用先排序的方法**，每次询问中位数的时候，都要先进行排序，那效率就不高了。

  #### 借助堆这种数据结构，我们不用排序，就可以非常高效地实现求中位数操作。我们来看看，它是如何做到的？

  我们需要维护两个堆，一个大顶堆，一个小顶堆。大顶堆中存储前半部分数据，小顶堆中存储后半部分数据，且小顶堆中的数据都大于大顶堆中的数据。

  也就是说，如果有 n 个数据，n 是偶数，我们从小到大排序，那前n/2 个数据存储在大顶堆中，后 n/2 个数据存储在小顶堆中。这样，大顶堆中的堆顶元素就是我们要找的中位数。如果 n 是奇数，情况是类似的，大顶堆就存储 (n/2)+1 个数据，小顶堆中就存储 n/2 个数据。

  <img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\顶对.webp" style="zoom:67%;" />

  我们前面也提到，数据是动态变化的，当新添加一个数据的时候，我们如何调整两个堆，让大顶堆中的堆顶元素继续是中位数呢？

  **如果新加入的数据小于等于大顶堆的堆顶元素，我们就将这个新数据插入到大顶堆**；**否则，我们就将这个新数据插入到小顶堆**。这个时候就有可能出现，两个堆中的数据个数不符合前面约定的情况：**如果 n 是偶数，两个堆中的数据个数都是 2n；如果 n 是奇数，大顶堆有 2n+1 个数据，小顶堆有 2n 个数据。这个时候，我们可以从一个堆中不停地将堆顶元素移动到另一个堆，通过这样的调整，来让两个堆中的数据满足上面的约定。**

  插入数据因为需要涉及堆化，所以时间复杂度变成了 O(logn)，但是求中位数我们只需要返回大顶堆的堆顶元素就可以了，所以时间复杂度就是 O(1)。

  

## 6. 图

- #### 如何理解“图”？

  我们前面讲过了树这种非线性表数据结构，今天我们要讲另一种非线性表数据结构，**图**（Graph）。和树比起来，这是一种更加复杂的非线性表结构。我们知道，树中的元素我们称为节点，图中的元素我们就叫做**顶点（vertex**）。从我画的图中可以看出来，图中的一个顶点可以与任意其他顶点建立连接关系。我们把这种建立的关系叫做**边（edge）**。

  <img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\图.webp" style="zoom:67%;" />

  

  **顶点的度（degree），就是跟顶点相连接的边的条数。**在有向图中，我们把度分为**入度（In-degree）**和**出度（Out-degree）**。顶点的入度，表示有多少条边指向这个顶点；顶点的出度，表示有多少条边是以这个顶点为起点指向其他顶点



### 图的存储

- #### 邻接矩阵存储方法

  邻接矩阵的底层依赖一个二维数组。对于无向图来说，如果顶点 i 与顶点 j 之间有边，我们就将 A[i][j]和 A[j][i]标记为 1；对于有向图来说，如果顶点 i 到顶点 j 之间，有一条箭头从顶点 i 指向顶点 j 的边，那我们就将 A[i][j]标记为 1。同理，如果有一条箭头从顶点 j 指向顶点 i 的边，我们就将 A[j][i]标记为 1。对于带权图，数组中就存储相应的权重。

  <img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\图存储.webp" style="zoom:67%;" />

  

- #### 邻接表存储方法

  针对上面邻接矩阵比较浪费内存空间的问题，我们来看另外一种图的存储方法，邻接表（Adjacency List）。

  图中画的是一个有向图的邻接表存储方式，每个顶点对应的链表里面，存储的是指向的顶点。

  <img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\邻接表.webp" style="zoom:67%;" />

  邻接矩阵存储起来比较浪费空间，但是使用起来比较节省时间。相反，邻接表存储起来比较节省空间，但是使用起来就比较耗时间。就像图中的例子，如果我们要确定，是否存在一条从顶点 2 到顶点 4 的边，那我们就要遍历顶点 2 对应的那条链表，看链表中是否存在顶点 4。而且，我们前面也讲过，链表的存储方式对缓存不友好。所以，比起邻接矩阵的存储方式，在邻接表中查询两个顶点之间的关系就没那么高效了。

  

  例如对于微博的粉丝关注：我们去查找某个用户关注了哪些用户非常容易，但是如果要想知道某个用户都被哪些用户关注了，也就是用户的粉丝列表，是非常困难的。基于此，我们需要一个逆邻接表。邻接表中存储了用户的关注关系，逆邻接表中存储的是用户的被关注关系。
  
  
  
  #### 使用邻接表进行存储
  
  ```go
  // Graph adjacency table 无向图结构体
  type Graph struct {
  	adj   []*list.List
  	value int
  }
  
  // NewGraph init graph according to capacity
  func NewGraph(value int) *Graph {
  	graph := &Graph{}
  	graph.value = value
  	graph.adj = make([]*list.List, value)
  	for i := range graph.adj {
  		graph.adj[i] = list.New()
  	}
  	return graph
  }
  
  // insert as add edge，一条边存2次
  func (g *Graph) addEdge(s int, t int) {
  	g.adj[s].PushBack(t)
  	g.adj[t].PushFront(s)
  }
  ```



### 广度优先搜索（BFS）

广度优先搜索（Breadth-First-Search），我们平常都简称 BFS。直观地讲，它其实就是一种“地毯式”层层推进的搜索策略，即先查找离起始顶点最近的，然后是次近的，依次往外搜索。

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\bfs.webp" style="zoom:67%;" />

图的广度优先搜索的代码实现。其中 s 表示起始顶点，t 表示终止顶点。我们搜索一条从 s 到 t 的路径。实际上，这样求得的路径就是从 s 到 t 的最短路径。

- #### **visited**

   是用来记录已经被访问的顶点，用来避免顶点被重复访问。如果顶点 q 被访问，那相应的 **visited[q]**会被设置为 **true**。

- #### queue 

  是一个队列，用来存储已经被访问、但相连的顶点还没有被访问的顶点。因为广度优先搜索是逐层访问的，也就是说，我们只有把第 k 层的顶点都访问完成之后，才能访问第 k+1 层的顶点。当我们访问到第 k 层的顶点的时候，我们需要把第 k 层的顶点记录下来，稍后才能通过第 k 层的顶点来找第 k+1 层的顶点。所以，我们用这个队列来实现记录的功能。

- #### prev 

  用来记录搜索路径。当我们从顶点 s 开始，广度优先搜索到顶点 t 后，prev 数组中存储的就是搜索的路径。不过，这个路径是反向存储的。prev[w]存储的是，顶点 w 是从哪个前驱顶点遍历过来的。比如，我们通过顶点 2 的邻接表访问到顶点 3，那 prev[3]就等于 2。为了正向打印出路径，我们需要递归地来打印，你可以看下 print() 函数的实现方式。

  <img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\分解图1.webp" style="zoom:67%;" />

  <img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\分解图2.webp" alt="分解图2" style="zoom:67%;" />

  <img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\分解图3.webp" alt="分解图3" style="zoom:67%;" />

  代码实现

  ```go
  // BFS 广度优先遍历
  func (g *Graph) BFS(s int, t int) {
  	if s == t {
  		return
  	}
  	// Init prev
  	prev := make([]int, g.value)
  	for index := range prev {
  		prev[index] = -1
  	}
  
  	// Search by queue
  	var queue []int
  	visited := make([]bool, g.value)
  	queue = append(queue, s)
  	visited[s] = true
  	isFound := false
  	for len(queue) > 0 && !isFound {
  		top := queue[0]
  		queue = queue[1:]
  		linkedList := g.adj[top]
  		for e := linkedList.Front(); e != nil; e = e.Next() {
  			k := e.Value.(int)
  			if visited[k] == false {
  				prev[k] = top
  				if k == t {
  					isFound = true
  					break
  				}
  				queue = append(queue, k)
  				visited[k] = true
  			}
  		}
  	}
  
  	if isFound {
  		printPrev(prev, s, t)
  	} else {
  		fmt.Printf("no path found from %d to %d\n", s, t)
  	}
  }
  ```

  - #### 广度优先搜索的时间、空间复杂度

    最坏情况下，终止顶点 t 离起始顶点 s 很远，需要遍历完整个图才能找到。

    这个时候，每个顶点都要进出一遍队列，每个边也都会被访问一次，所以，广度优先搜索的时间复杂度是 **O(V+E)**，其中，**V 表示顶点的个数，E 表示边的个数**。当然，对于一个连通图来说，也就是说一个图中的所有顶点都是连通的，E 肯定要大于等于 V-1，所以，广度优先搜索的时间复杂度也可以简写为 O(E)。广度优先搜索的空间消耗主要在几个辅助变量 visited 数组、queue 队列、**prev** 数组上。这三个存储空间的大小都不会超过顶点的个数，所以空间复杂度是 O(V)。





### 深度优先搜索（DFS）

你可以看我画的这幅图。搜索的起始顶点是 s，终止顶点是 t，我们希望在图中寻找一条从顶点 s 到顶点 t 的路径。如果映射到迷宫那个例子，s 就是你起始所在的位置，t 就是出口。我用深度递归算法，把整个搜索的路径标记出来了。这里面实线箭头表示遍历，虚线箭头表示回退。从图中我们可以看出，深度优先搜索找出来的路径，并不是顶点 s 到顶点 t 的最短路径。

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\dfs.webp" style="zoom:67%;" />

```go
func (g *Graph) DFS(s, t int) {
	prev := make([]int, g.value)
	for index := range prev {
		prev[index] = -1
	}

	visited := make([]bool, g.value)
	visited[s] = true
	isFound := false
	g.recurse(s, t, prev, visited, isFound)

}

func (g *Graph) recurse(s, t int, prev []int, visited []bool, isFound bool) {
	if isFound {
		return
	}
	visited[s] = true

	if s == t {
		isFound = true
		return
	} 
	linkedList := g.adj[s]
	for e := linkedList.Front(); e != nil; e = e.Next() {
		k := e.Value.(int)
		if !visited[k] {
			prev[k] = s
			g.recurse(k, t, prev, visited, false)
		}
	}
}
```





- #### 深度优先搜索的时间、空间复杂度

  从我前面画的图可以看出，每条边最多会被访问两次，一次是遍历，一次是回退。所以，图上的深度优先搜索算法的时间复杂度是 O(E)，E 表示边的个数。深度优先搜索算法的消耗内存主要是 visited、prev 数组和递归调用栈。visited、prev 数组的大小跟顶点的个数 V 成正比，递归调用栈的最大深度不会超过顶点的个数，所以总的空间复杂度就是 O(V)。、



## 7.字符串匹配算法

### 单模式串匹配的算法

#### BF算法

​		也就是一个串跟一个串进行匹配，**RK 算法是 BF 算法的改进，它巧妙借助了我们前面讲过的哈希算法，让匹配的效率有了很大的提升。**

​		BF 算法BF 算法中的 BF 是 Brute Force 的缩写，中文叫作暴力匹配算法，也叫朴素匹配算法。从名字可以看出，这种算法的字符串匹配方式很“暴力”，当然也就会比较简单、好懂，但相应的性能也不高。在开始讲解这个算法之前，我先定义两个概念，方便我后面讲解。它们分别是**主串**和**模式串**。这俩概念很好理解，我举个例子你就懂了。

​		比方说，我们在**字符串 A 中查找字符串 B**，那字符串 **A 就是主串**，字符串 **B 就是模式串**。我们把主串的长度记作 n，模式串的长度记作 m。因为我们是在主串中查找模式串，所以 n>m。

​		BF 算法的思想可以用一句话来概括，那就是，我们在主串中，检查起始位置分别是 0、1、2....n-m 且长度为 m 的 n-m+1 个子串，看有没有跟模式串匹配的：

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\bf算法.webp" style="zoom:67%;" />

​		在极端情况下，比如主串是“aaaaa....aaaaaa”（省略号表示有很多重复的字符 a），模式串是“aaaaab”。我们每次都比对 m 个字符，要比对 n-m+1 次，所以，这种算法的最坏情况时间复杂度是 O(n*m)。

​		尽管理论上，BF 算法的时间复杂度很高，是 O(n*m)，但在实际的开发中，它却是一个比较常用的字符串匹配算法。为什么这么说呢？原因有两点。

​		第一，**实际的软件开发中**，大部分情况下，**模式串和主串的长度都不会太长**。而且每次模式串与主串中的子串匹配的时候，当中途遇到不能匹配的字符的时候，就可以就停止了，不需要把 m 个字符都比对一下。所以，尽管理论上的最坏情况时间复杂度是 O(nm)，但是，统计意义上，大部分情况下，算法执行效率要比这个高很多。

​		第二，**朴素字符串匹配算法思想简单，代码实现也非常简单**。简单意味着不容易出错，如果有 bug 也容易暴露和修复。在工程中，在满足性能要求的前提下，简单是首选。这也是我们常说的KISS（Keep it Simple and Stupid）设计原则。所以，在实际的软件开发中，绝大部分情况下，朴素的字符串匹配算法就够用了。



#### RK 算法

RK 算法的全称叫 Rabin-Karp 算法，是由它的两位发明者 Rabin 和 Karp 的名字来命名的。这个算法理解起来也不是很难。我个人觉得，它其实就是刚刚讲的 BF 算法的升级版。

我在讲 BF 算法的时候讲过，**如果模式串长度为 m，主串长度为 n，那在主串中，就会有 n-m+1 个长度为 m 的子串**，我们只需要暴力地对比这 n-m+1 个子串与模式串，就可以找出主串与模式串匹配的子串。

但是，每次检查主串与子串是否匹配，需要依次比对每个字符，所以 BF 算法的时间复杂度就比较高，是 O(n*m)。我们对朴素的字符串匹配算法稍加改造，引入哈希算法，时间复杂度立刻就会降低。

**RK 算法的思路是这样的：**

​		**我们通过哈希算法对主串中的 n-m+1 个子串分别求哈希值**，然后逐个与模式串的哈希值比较大小。如果某个子串的哈希值与模式串相等，那就说明对应的子串和模式串匹配了（这里先不考虑哈希冲突的问题，后面我们会讲到）。因为哈希值是一个数字，数字之间比较是否相等是非常快速的，所以模式串和子串比较的效率就提高了。

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\hash.webp" style="zoom: 50%;" />

**举个例子：**

​		比如要处理的字符串只包含 **a～z 这 26 个小写字母**，那我们就用二十六进制来表示一个字符串。我们把 **a～z 这 26 个字符映射到 0～25 这 26 个数字**，a 就表示 1，b 就表示 2，以此类推，z 表示 26。在十进制的表示法中，一个数字的值是通过下面的方式计算出来的。对应到二十六进制，一个包含 a 到 z 这 26 个字符的字符串，计算哈希的时候，我们只需要把进位从 10 改成 26 就可以。

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\hash进制.webp" style="zoom: 50%;" />

​		我们很容易就能得出这样的规律：相邻两个子串 s[i-1]和 s[i]（i 表示子串在主串中的起始位置，子串的长度都为 m），对应的哈希值计算公式有交集，也就是说，我们可以使用 s[i-1]的哈希值很快的计算出 s[i]的哈希值。如果用公式表示的话，就是下面这个样子：

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\hash规律.webp" style="zoom: 25%;" />

不过，这里有一个小细节需要注意，那就是 26^(m-1) 这部分的计算，我们可以通过查表的方法来提高效率。我们事先计算好 26^0、26^1、26^2……26^(m-1)，并且存储在一个长度为 m 的数组中，公式中的“次方”就对应数组的下标。当我们需要计算 26 的 x 次方的时候，就可以从数组的下标为 x 的位置取值，直接使用，省去了计算的时间。

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\次方.webp" style="zoom: 50%;" />



#### BM算法

我们就来学习 BM（Boyer-Moore）算法。它是一种非常高效的字符串匹配算法，有实验统计，它的性能是著名的KMP 算法的 3 到 4 倍。BM 算法的原理很复杂，比较难懂。

- #### BM 算法的核心思想


​		我们把模式串和主串的匹配过程，看作模式串在主串中不停地往后滑动。当遇到不匹配的字符时，BF 算法和 RK 算法的做法是，模式串往后滑动一位，然后从模式串的第一个字符开始重新匹配。我举个例子解释一下，你可以看我画的这幅图。

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\移动匹配.webp" style="zoom: 50%;" />

在这个例子里，**主串中的 c，在模式串中是不存在的**，所以，模式串向后滑动的时候，只要 c 与模式串没有重合，肯定无法匹配。所以，我们可以一次性把模式串往后多滑动几位，把模式串移动到 c 的后面。

BM 算法包含两部分，**分别是坏字符规则（bad character rule）和好后缀规则（good suffix shift）**。我们下面依次来看，这两个规则分别都是怎么工作的。

- ####  坏字符规则

前面两节讲的算法，在匹配的过程中，我们都是按模式串的下标从小到大的顺序，依次与主串中的字符进行匹配的。这种匹配顺序比较符合我们的思维习惯，而 BM 算法的匹配顺序比较特别，**它是按照模式串下标从大到小的顺序，倒着匹配的**。我画了一张图，你可以看下。

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\匹配顺序1.webp" style="zoom: 50%;" />

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\匹配顺序2.webp" alt="匹配顺序2" style="zoom: 50%;" />

从模式串的末尾往前倒着匹配，**当发现某个字符没法匹配的时候，我们把这个没有匹配的字符叫作坏字符（主串中的字符）**。

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\坏字符.webp" style="zoom: 50%;" />

我们拿坏字符 c 在模式串中查找，**发现模式串中并不存在这个字符**，也就是说，**字符 c 与模式串中的任何字符都不可能匹配。这个时候，我们可以将模式串直接往后滑动三位，将模式串滑动到 c 后面的位置，再从模式串的末尾字符开始比较。**

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\滑动.webp" style="zoom: 50%;" />

这个时候，我们发现，模式串中最后一个字符 d，还是无法跟主串中的 a 匹配，这个时候，还能将模式串往后滑动三位吗？答案是不行的。因为这个时候，坏字符 a 在模式串中是存在的，模式串中下标是 0 的位置也是字符 a。**这种情况下，我们可以将模式串往后滑动两位，让两个 a 上下对齐，然后再从模式串的末尾字符开始，重新匹配。**

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\重新匹配.webp" style="zoom: 50%;" />

当发生不匹配的时候，我们把坏字符对应的模式串中的字符下标记作 **si**。如果坏字符在模式串中存在，我们把这个坏字符在模式串中的下标记作 **xi**。如果不存在，我们把 **xi** 记作 -1。那模式串往后移动的位数就等于 **si-xi**。（注意，我这里说的下标，都是字符在模式串的下标）。

如果坏字符在模式串里多处出现，那我们在计算 **xi** 的时候，**选择最靠后的那个，因为这样不会让模式串滑动过多，导致本来可能匹配的情况被滑动略过。**

利用坏字符规则，BM 算法在最好情况下的时间复杂度非常低，是 O(n/m)。比如，主串是 `aaabaaabaaabaaab`，模式串是 `aaaa`。每次比对，模式串都可以直接后移四位，所以，**匹配具有类似特点的模式串和主串的时候，BM 算法非常高效**。

不过，单纯使用坏字符规则还是不够的。因为根据 si-xi 计算出来的移动位数，有可能是负数，比如主串是 `aaaaaaaaaaaaaaaa`，模式串是 `baaa`。**不但不会向后滑动模式串，还有可能倒退。所以，BM 算法还需要用到“好后缀规则”。**

- #### 好后缀规则

​		好后缀规则实际上跟坏字符规则的思路很类似。你看我下面这幅图。**当模式串滑动到图中的位置的时候，模式串和主串有 2 个字符是匹配的，倒数第 3 个字符发生了不匹配的情况**。

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\好后缀.webp" style="zoom: 50%;" />

这个时候该如何滑动模式串呢？当然，我们还可以利用坏字符规则来计算模式串的滑动位数，不过，我们也可以使用好后缀处理规则。两种规则到底如何选择，我稍后会讲。抛开这个问题，现在我们来看，好后缀规则是怎么工作的？

**我们把已经匹配的 bc 叫作好后缀**，记作{u}。我们拿它在模式串中查找，如果找到了另一个跟{u}相匹配的子串{u*}，那我们就将模式串滑动到子串{u*}与主串中{u}对齐的位置。

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\好后缀滑动.webp" style="zoom: 50%;" />

如果在模式**串中找不到另一个等于{u}的子串**，**我们就直接将模式串，滑动到主串中{u}的后面**，**因为之前的任何一次往后滑动，都没有匹配主串中{u}的情况**。

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\后缀不匹配.webp" style="zoom: 50%;" />

不过，当模式串中不存在等于{u}的子串时，我们直接将模式串滑动到主串{u}的后面。这样做是否有点太过头呢？我们来看下面这个例子。这里面 bc 是好后缀，尽管在模式串中没有另外一个相匹配的子串{u*}，但是如果我们将模式串移动到好后缀的后面，如图所示，那就会错过模式串和主串可以匹配的情况。

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\过度滑动.webp" style="zoom: 50%;" />

如果**好后缀在模式串中不存在可匹配的子串**，**那在我们一步一步往后滑动模式串的过程中，只要主串中的{u}与模式串有重合，那肯定就无法完全匹配。但是当模式串滑动到前缀与主串中{u}的后缀有部分重合的时候，并且重合的部分相等的时候，就有可能会存在完全匹配的情况**。

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\珠串模式串.webp" style="zoom: 50%;" />

所以，针对这种情况，**我们不仅要看好后缀在模式串中，是否有另一个匹配的子串**，**我们还要考察好后缀的后缀子串，是否存在跟模式串的前缀子串匹配的。**所谓某个字符串 s 的后缀子串，就是最后一个字符跟 s 对齐的子串，比如 abc 的后缀子串就包括 c, bc。所谓前缀子串，就是起始字符跟 s 对齐的子串，比如 abc 的前缀子串有 a，ab。我们从好后缀的后缀子串中，找一个最长的并且能跟模式串的前缀子串匹配的，假设是{v}，然后将模式串滑动到如图所示的位置。

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\滑动位置.webp" style="zoom: 50%;" />

**当模式串和主串中的某个字符不匹配的时候，如何选择用好后缀规则还是坏字符规则，来计算模式串往后滑动的位数？**

我们可以分别计算好后缀和坏字符往后滑动的位数，**然后取两个数中最大的**，作为模式串往后滑动的位数。这种处理方法还可以避免我们前面提到的，根据坏字符规则，计算得到的往后滑动的位数，有可能是负数的情况。

- ### BM 算法代码实现

  “坏字符规则”本身不难理解。当遇到坏字符时，要计算往后移动的位数 **si-xi**，其中 **xi** 的计算是重点，我们如何求得 xi 呢？

  或者说，如何查找坏字符在模式串中出现的位置呢？**如果我们拿坏字符，在模式串中顺序遍历查找，这样就会比较低效，势必影响这个算法的性能。**有没有更加高效的方式呢？

  **我们之前学的散列表，这里可以派上用场了**。**我们可以将模式串中的每个字符及其下标都存到散列表中**。这样就可以快速找到坏字符在模式串的位置下标了。关于这个散列表，我们只实现一种最简单的情况，假设字符串的字符集不是很大，每个字符长度是 1 字节，**我们用大小为 256 的数组，来记录每个字符在模式串中出现的位置。数组的下标对应字符的 ASCII 码值**，**数组中存储这个字符在模式串中出现的位置**。

  如果将上面的过程翻译成代码，就是下面这个样子。其中，变量 b 是模式串，m 是模式串的长度，bc 表示刚刚讲的散列表。

  ```go
  const SIZE = 256
  func generateBC(b []string,bc []string,m int){
      for i:=0;i<SIZE;i++{
          bc[i] = -1// 初始化bc
      }
      for i:=0;i<m;i++{
          ascii := b[i].(int)
          bc[ascii] = i
      }
  }
  ```

  掌握了坏字符规则之后，我们先把 BM 算法代码的大框架写好，先不考虑好后缀规则，仅用坏字符规则，并且不考虑 si-xi 计算得到的移动位数可能会出现负数的情况。

  ```go
  func BMSearch(a []int, b []int, n, m int) int {
  	bc := make([]int, SIZE) // 记录模式串中每个字符最后出现的位置
  	// 构建坏字符哈希表
  	generateBC(b, bc, m)
  	i := 0
  	for i <= n-m {
  		var j int
  		for j := m - 1; j >= 0; j-- {
  			if a[i+j] != b[j] {
  				break
  			}
  		}
  		if j < 0 {
  			return i
  		}
  		i = i + (j - bc[int(a[i+j])])
  	}
  	return -1
  }
  ```

  <img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\代码.webp" style="zoom:50%;" />

  

  **好后缀的处理规则中最核心的内容：**

  - 在模式串中，查找跟好后缀匹配的另一个子串；
  - 在好后缀的后缀子串中，查找最长的、能跟模式串前缀子串匹配的后缀子串；

  如何表示模式串中不同的后缀子串呢？因为后缀子串的最后一个字符的位置是固定的，下标为 m-1，我们只需要记录长度就可以了。通过长度，我们可以确定一个唯一的后缀子串。

  <img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\后缀子串.webp" style="zoom:50%;" />

  现在，我们要引入最关键的变量 suffix 数组。suffix 数组的下标 k，表示后缀子串的长度，下标对应的数组值存储的是，在模式串中跟好后缀{u}相匹配的子串{u*}的起始下标值。这句话不好理解，我举一个例子。

  <img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\suffix.webp" style="zoom:50%;" />

  但是，如果模式串中有多个（大于 1 个）子串跟后缀子串{u}匹配，那 suffix 数组中该存储哪一个子串的起始位置呢？为了避免模式串往后滑动得过头了，我们肯定要存储模式串中最靠后的那个子串的起始位置，也就是下标最大的那个子串的起始位置。不过，这样处理就足够了吗？

  

  我们不仅要在模式串中，**查找跟好后缀匹配的另一个子串**，**还要在好后缀的后缀子串中，查找最长的能跟模式串前缀子串匹配的后缀子串**。如果我们只记录刚刚定义的 suffix，实际上，只能处理规则的前半部分，也就是，在模式串中，查找跟好后缀匹配的另一个子串。所以**，除了 suffix 数组之外，我们还需要另外一个 bool 类型的 prefix 数组，来记录模式串的后缀子串是否能匹配模式串的前缀子串。**

  <img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\prefix.webp" style="zoom:50%;" />

  现在，我们来看下，如何来计算并填充这两个数组的值？这个计算过程非常巧妙。

  **我们拿下标从 0 到 i 的子串（i 可以是 0 到 m-2）与整个模式串，求公共后缀子串**。如果公共后缀子串的长度是 k，那我们就记录 suffix[k]=j（j 表示公共后缀子串的起始下标）。如果 j 等于 0，也就是说，公共后缀子串也是模式串的前缀子串，我们就记录 prefix[k]=true。

  <img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\长度为.webp" style="zoom:50%;" />

  ```go
  // b表示模式串，m表示长度，suffix，prefix数组事先申请好了
  func generateGS(b []string, m int, suffix []int, prefix []bool) {
  	// 初始化
  	for i := 0; i < m; i++ {
  		prefix[i] = false
  		suffix[i] = -1
  	}
  	// b[0,i]
  	for i := 0; i < m-1; i++ {
  		j := i
  		// 公共后缀子串长度
  		k := 0
  		for j >= 0 && b[j] == b[m-1-k] { //与b[0,m-1]求公共后缀子串
  			j--
  			k++
  			suffix[k] = j + 1 //j+1表示公共后缀子串在b[0, i]中的起始下标
  		}
  		if j == -1 {
  			prefix[k] = true
  		}
  	}
  }
  ```

  有了这两个数组之后，我们现在来看，在模式串跟主串匹配的过程中，遇到不能匹配的字符时，如何根据好后缀规则，计算模式串往后滑动的位数？

  假设好后缀的长度是 k。**我们先拿好后缀，在 suffix 数组中查找其匹配的子串。如果 suffix[k]不等于 -1（-1 表示不存在匹配的子串），那我们就将模式串往后移动 j-suffix[k]+1 位（j 表示坏字符对应的模式串中的字符下标）。如果 suffix[k]等于 -1，表示模式串中不存在另一个跟好后缀匹配的子串片段。我们可以用下面这条规则来处理。**

  <img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\匹配规则.webp" style="zoom:50%;" />

  好后缀的后缀子串 b[r, m-1]（其中，r 取值从 j+2 到 m-1）的长度 k=m-r，如果 prefix[k]等于 true，表示长度为 k 的后缀子串，有可匹配的前缀子串，这样我们可以把模式串后移 r 位。

  <img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\规则1.webp" style="zoom:50%;" />

  **如果两条规则都没有找到可以匹配好后缀及其后缀子串的子串，我们就将整个模式串后移 m 位。**



#### KMP算法

KMP 算法的核心思想，跟上一节讲的 BM 算法非常相近。我们假设主串是 a，模式串是 b。在模式串与主串匹配的过程中，当遇到不可匹配的字符的时候，我们希望找到一些规律，可以将模式串往后多滑动几位，跳过那些肯定不会匹配的情况。

这里我们可以类比一下，在模式串和主串匹配的过程中，**把不能匹配的那个字符仍然叫作坏字符，把已经匹配的那段字符串叫作好前缀。**

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\好前缀坏字符.webp" style="zoom:50%;" />

当遇到坏字符的时候，我们就要把模式串往后滑动，**在滑动的过程中，只要模式串和好前缀有上下重合，前面几个字符的比较，就相当于拿好前缀的后缀子串，跟模式串的前缀子串在比较。**这个比较的过程能否更高效了呢？可以不用一个字符一个字符地比较了吗？

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\好前缀子串.webp" style="zoom:50%;" />

KMP 算法就是在试图寻找一种规律：在模式串和主串匹配的过程中，**当遇到坏字符后，对于已经比对过的好前缀，能否找到一种规律，将模式串一次性滑动很多位？我们只需要拿好前缀本身，在它的后缀子串中，查找最长的那个可以跟好前缀的前缀子串匹配的。假设最长的可匹配的那部分前缀子串是{v}，长度是 k。我们把模式串一次性往后滑动 j-k 位，相当于，每次遇到坏字符的时候，我们就把 j 更新为 k，i 不变，然后继续比较。**

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\继续比对1.webp" style="zoom:50%;" />

为了表述起来方便，我把好前缀的所有后缀子串中，最长的可匹配前缀子串的那个后缀子串，叫作最长可匹配后缀子串；对应的前缀子串，叫作最长可匹配前缀子串。

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\继续比对2.webp" style="zoom:50%;" />

如何来求好前缀的最长可匹配前缀和后缀子串

我发现，这个问题其实不涉及主串，只需要通过模式串本身就能求解。所以，我就在想，能不能事先预处理计算好，在模式串和主串匹配的过程中，直接拿过来就用呢？类似 BM 算法中的 bc、suffix、prefix 数组，KMP 算法也可以提前构建一个数组，用来存储模式串中每个前缀（这些前缀都有可能是好前缀）的最长可匹配前缀子串的结尾字符下标。**我们把这个数组定义为 next 数组，很多书中还给这个数组起了一个名字，叫失效函数（failure function）。数组的下标是每个前缀结尾字符下标，数组的值是这个前缀的最长可以匹配前缀子串的结尾字符下标。这句话有点拗口，我举了一个例子，你一看应该就懂了。**

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\next数组.webp" style="zoom:50%;" />

有了 next 数组，我们很容易就可以实现 KMP 算法了。我先假设 next 数组已经计算好了，先给出 KMP 算法的框架代码。

```go
func KMP(main string, pattern string) int {
	n := len(main)
	m := len(pattern)
	if n < m {
		return -1
	}
	next := getNext(pattern)
	j := 0
	for i := 0; i < n; i++ {
		for j > 0 && main[i] != pattern[j] {
			j = next[j-1] + 1
		}
		if main[i] == pattern[j] {
			if j == m-1 {
				return i - m + 1
			}
			j++
		}
	}
	return -1
}
```

- #### 失效函数计算方法

  我们按照下标从小到大，依次计算 next 数组的值。当我们要计算 next[i]的时候，前面的 next[0]，next[1]，……，next[i-1]应该已经计算出来了。利用已经计算出来的 next 值，我们是否可以快速推导出 next[i]的值呢？

  如果 next[i-1]=k-1，也就是说，子串 b[0, k-1]是 b[0, i-1]的最长可匹配前缀子串。**如果子串 b[0, k-1]的下一个字符 b[k]，与 b[0, i-1]的下一个字符 b[i]匹配，那子串 b[0, k]就是 b[0, i]的最长可匹配前缀子串。所以，next[i]等于 k。**但是，如果 b[0, k-1]的下一字符 b[k]跟 b[0, i-1]的下一个字符 b[i]不相等呢？这个时候就不能简单地通过 next[i-1]得到 next[i]了。这个时候该怎么办呢？

  <img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\nextshuzu1.webp" style="zoom:50%;" />

  ​		我们假设 b[0, i]的最长可匹配后缀子串是 b[r, i]。如果我们把最后一个字符去掉，那 b[r, i-1]肯定是 b[0, i-1]的可匹配后缀子串，但不一定是最长可匹配后缀子串。所以，既然 b[0, i-1]最长可匹配后缀子串对应的模式串的前缀子串的下一个字符并不等于 b[i]，那么我们就可以考察 b[0, i-1]的次长可匹配后缀子串 b[x, i-1]对应的可匹配前缀子串 b[0, i-1-x]的下一个字符 b[i-x]是否等于 b[i]。如果等于，那 b[x, i]就是 b[0, i]的最长可匹配后缀子串。

  <img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\kmp-next.webp" style="zoom:50%;" />

  可是，如何求得 b[0, i-1]的次长可匹配后缀子串呢？次长可匹配后缀子串肯定被包含在最长可匹配后缀子串中，而最长可匹配后缀子串又对应最长可匹配前缀子串 b[0, y]。于是，查找 b[0, i-1]的次长可匹配后缀子串，这个问题就变成，查找 b[0, y]的最长匹配后缀子串的问题了。

  <img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\kmp-next-2.webp" style="zoom:50%;" />

  按照这个思路，我们可以考察完所有的 b[0, i-1]的可匹配后缀子串 b[y, i-1]，直到找到一个可匹配的后缀子串，它对应的前缀子串的下一个字符等于 b[i]，那这个 b[y, i]就是 b[0, i]的最长可匹配后缀子串。前面我已经给出 KMP 算法的框架代码了，现在我把这部分的代码也写出来了。这两部分代码合在一起，就是整个 KMP 算法的代码实现。

  

  1、次长子串是模版串的一个前缀子串 

  2、最长子串也是模版串的一个前缀子串 

  3、次长子串是最长子串的一个部分 

  得出：次长子串也是最长子串的一个前缀子串。。。 刚好可以借助之前填充的next数组找到（最长子串之前也作为好前缀候选，放到next数组里过） 因此 k = next[k]

  ```go
  func getNexts(pattern string) []int {
  	m := len(pattern)
  	nexts := make([]int, m)
  	for index := range nexts {
  		nexts[index] = -1
  	}
  
  	for i := 1; i < m - 1; i++ {
  		j := nexts[i - 1]
  
  		for pattern[j + 1] != pattern[i] && j >= 0 {
  			j = nexts[j]
  		}
  
  		if pattern[j + 1] == pattern[i] {
  			j += 1
  		}
  
  		nexts[i] = j
  	}
  
  	return nexts
  }
  ```



- #### KMP 算法复杂度分析

  KMP 算法的原理和实现我们就讲完了，我们现在来分析一下 KMP 算法的时间、空间复杂度是多少？

  空间复杂度很容易分析，KMP 算法只需要一个额外的 next 数组，数组的大小跟模式串相同。**所以空间复杂度是 O(m)，m 表示模式串的长度。**

  KMP 算法包含两部分，第一部分是构建 next 数组，第二部分才是借助 next 数组匹配。所以，关于时间复杂度，我们要分别从这两部分来分析。

  我们先来分析第一部分的时间复杂度。计算 next 数组的代码中，第一层 for 循环中 i 从 1 到 m-1，也就是说，内部的代码被执行了 m-1 次。for 循环内部代码有一个 while 循环，如果我们能知道每次 for 循环、while 循环平均执行的次数，假设是 k，那时间复杂度就是 O(k*m)。但是，while 循环执行的次数不怎么好统计，所以我们放弃这种分析方法。我们可以找一些参照变量，i 和 k。**i 从 1 开始一直增加到 m，而 k 并不是每次 for 循环都会增加，所以，k 累积增加的值肯定小于 m。而 while 循环里 k=next[k]，实际上是在减小 k 的值，k 累积都没有增加超过 m，所以 while 循环里面 k=next[k]总的执行次数也不可能超过 m。因此，next 数组计算的时间复杂度是 O(m)。**我们再来分析第二部分的时间复杂度。分析的方法是类似的。i 从 0 循环增长到 n-1，j 的增长量不可能超过 i，所以肯定小于 n。而 while 循环中的那条语句 j=next[j-1]+1，不会让 j 增长的，那有没有可能让 j 不变呢？也没有可能**。因为 next[j-1]的值肯定小于 j-1，所以 while 循环中的这条语句实际上也是在让 j 的值减少。而 j 总共增长的量都不会超过 n，那减少的量也不可能超过 n，所以 while 循环中的这条语句总的执行次数也不会超过 n，所以这部分的时间复杂度是 O(n)。所以，综合两部分的时间复杂度，KMP 算法的时间复杂度就是 O(m+n)。**

​	



## 8.Trie树

Trie 树，也叫“字典树”。顾名思义，它是一个树形结构。**它是一种专门处理字符串匹配的数据结构，用来解决在一组字符串集合中快速查找某个字符串的问题**。

**Trie 树的本质，就是利用字符串之间的公共前缀，将重复的前缀合并在一起**

我们有 6 个字符串，它们分别是：how，hi，her，hello，so，see。这个时候，我们就可以先对这 6 个字符串做一下预处理，组织成 Trie 树的结构，之后每次查找，都是在 Trie 树中进行匹配查找。最后构造出来的就是下面这个图中的样子。

其中，**根节点不包含任何信息。每个节点表示一个字符串中的字符，从根节点到红色节点的一条路径表示一个字符串（注意：红色节点并不都是叶子节点）**。

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\trie树.webp" style="zoom:50%;" />

构造过程的每一步，都相当于往 Trie 树中插入一个字符串。当所有字符串都插入完成之后，Trie 树就构造好了。

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\构建1.webp" style="zoom:50%;" />

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\构建2.webp" style="zoom:50%;" />

当我们在 Trie 树中查找一个字符串的时候，比如查找字符串“her”，**那我们将要查找的字符串分割成单个的字符 h，e，r，然后从 Trie 树的根节点开始匹配。如图所示，绿色的路径就是在 Trie 树中匹配的路径。**

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\tire-find.webp" style="zoom:50%;" />

### 实现一棵 Trie 树

知道了 Trie 树长什么样子，我们现在来看下，如何用代码来实现一个 Trie 树。

从刚刚 Trie 树的介绍来看，Trie 树主要有两个操作：

- 一个是将字符串集合构造成 Trie 树。这个过程分解开来的话，就是一个将字符串插入到 Trie 树的过程。
- 另一个是在 Trie 树中查询一个字符串。

了解了 Trie 树的两个主要操作之后，我们再来看下，如何存储一个 Trie 树？

从前面的图中，我们可以看出，Trie 树是一个多叉树。我们知道，**二叉树中**，**一个节点的左右子节点是通过两个指针来存储的**，那对于多叉树来说，我们怎么存储一个节点的所有子节点的指针呢？

```go
type BinaryTreeNode struct{
	left  *BinaryTreeNode
	right *BinaryTreeNode
	value string
}
```

我先介绍其中一种存储方式，也是经典的存储方式，大部分数据结构和算法书籍中都是这么讲的。还记得我们前面讲到的散列表吗？**借助散列表的思想，我们通过一个下标与字符一一映射的数组，来存储子节点的指针。**这句话稍微有点抽象，不怎么好懂，我画了一张图你可以看看。

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\字符数组.webp" style="zoom:50%;" />

假设我们的字符串中只有从 a 到 z 这 26 个小写字母，**我们在数组中下标为 0 的位置，存储指向子节点 a 的指针，下标为 1 的位置存储指向子节点 b 的指针，以此类推，下标为 25 的位置，存储的是指向的子节点 z 的指针。如果某个字符的子节点不存在，我们就在对应的下标的位置存储 null。**

```go
type TrieNode struct {
	data string
	children []TrieNode
}
```

当我们在 Trie 树中查找字符串的时候，我们就可以通过字符的 ASCII 码减去“a”的 ASCII 码，迅速找到匹配的子节点的指针。比如，d 的 ASCII 码减去 a 的 ASCII 码就是 3，那子节点 d 的指针就存储在数组中下标为 3 的位置中。



### 时间复杂度分析

如果要在一组字符串中，频繁地查询某些字符串，用 Trie 树会非常高效。

构建 Trie 树的过程，需要扫描所有的字符串，**时间复杂度是 O(n)（n 表示所有字符串的长度和）。**

但是一旦构建成功之后，后续的查询操作会非常高效。每次查询时，**如果要查询的字符串长度是 k**，那我们只需要比对大约 k 个节点，就能完成查询操作。跟原本那组字符串的长度和个数没有任何关系。**所以说，构建好 Trie 树后，在其中查找字符串的时间复杂度是 O(k)，k 表示要查找的字符串的长度。**



### Trie 树真的很耗内存吗？

前面我们讲了 Trie 树的实现，也分析了时间复杂度。现在你应该知道，Trie 树是一种非常独特的、高效的字符串匹配方法。但是，关于 Trie 树，你有没有听过这样一种说法：“Trie 树是非常耗内存的，用的是一种空间换时间的思路”。

这是什么原因呢？刚刚我们在讲 Trie 树的实现的时候，讲到用数组来存储一个节点的子节点的指针。**如果字符串中包含从 a 到 z 这 26 个字符，那每个节点都要存储一个长度为 26 的数组，并且每个数组元素要存储一个 8 字节指针（或者是 4 字节，这个大小跟 CPU、操作系统、编译器等有关）。而且，即便一个节点只有很少的子节点，远小于 26 个，比如 3、4 个，我们也要维护一个长度为 26 的数组。**





## 9.贪心算法

- #### 贪心算法解决问题的步骤

  - **第一步，当我们看到这类问题的时候，首先要联想到贪心算法：**针对一组数据，我们定义了限制值和期望值，希望从中选出几个数据，在满足限制值的情况下，期望值最大。类比到刚刚的例子，限制值就是重量不能超过 100kg，期望值就是物品的总价值。这组数据就是 5 种豆子。我们从中选出一部分，满足重量不超过 100kg，并且总价值最大。

  - **第二步，我们尝试看下这个问题是否可以用贪心算法解决：**每次选择当前情况下，在对限制值同等贡献量的情况下，对期望值贡献最大的数据。类比到刚刚的例子，我们每次都从剩下的豆子里面，选择单价最高的，也就是重量相同的情况下，对价值贡献最大的豆子。

  - **第三步，我们举几个例子看下贪心算法产生的结果是否是最优的。**大部分情况下，举几个例子验证一下就可以了。严格地证明贪心算法的正确性，是非常复杂的，需要涉及比较多的数学推理。而且，从实践的角度来说，大部分能用贪心算法解决的问题，贪心算法的正确性都是显而易见的，也不需要严格的数学推导证明。

实际上，用贪心算法解决问题的思路，并不总能给出最优解。

**举个例子**

在一个有权图中，我们从顶点 S 开始，找一条到顶点 T 的最短路径（路径中边的权值和最小）。贪心算法的解决思路是，**每次都选择一条跟当前顶点相连的权最小的边，直到找到顶点 T。**按照这种思路，我们求出的最短路径是 S->A->E->T，路径长度是 1+4+4=9。

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\有权图.webp" style="zoom:50%;" />

但是，这种贪心的选择方式，最终求的路径并不是最短路径**，因为路径 S->B->D->T 才是最短路径，因为这条路径的长度是 2+2+2=6。为什么贪心算法在这个问题上不工作了呢？**在这个问题上，贪心算法不工作的主要原因是**，前面的选择，会影响后面的选择。如果我们第一步从顶点 S 走到顶点 A，那接下来面对的顶点和边，跟第一步从顶点 S 走到顶点 B，是完全不同的。所以，即便我们第一步选择最优的走法（边最短），但有可能因为这一步选择，导致后面每一步的选择都很糟糕，最终也就无缘全局最优解了。**



### 贪心算法实战分析

- #### 分糖果

  ​		我们有 m 个糖果和 n 个孩子。

  ​		我们现在要把糖果分给这些孩子吃，但是糖果少，孩子多（m<n），所以糖果只能分配给一部分孩子。每个糖果的大小不等，**这 m 个糖果的大小分别是 s1，s2，s3，……，sm。**

  ​		除此之外，每个孩子对糖果大小的需求也是不一样的，**只有糖果的大小大于等于孩子的对糖果大小的需求的时候，孩子才得到满足。**假设这 n 个孩子对糖果大小的需求分别是 **g1，g2，g3，……，gn**。

  ​		我的问题是，**如何分配糖果，能尽可能满足最多数量的孩子**？

  ​		我们可以把这个问题抽象成，**从 n 个孩子中，抽取一部分孩子分配糖果，让满足的孩子的个数（期望值）是最大的。这个问题的限制值就是糖果个数 m。**我们现在来看看如何用贪心算法来解决。对于一个孩子来说，如果小的糖果可以满足，我们就没必要用更大的糖果，这样更大的就可以留给其他对糖果大小需求更大的孩子。另一方面，对糖果大小需求小的孩子更容易被满足，所以，我们可以从需求小的孩子开始分配糖果。因为满足一个需求大的孩子跟满足一个需求小的孩子，对我们期望值的贡献是一样的。**我们每次从剩下的孩子中，找出对糖果大小需求最小的，然后发给他剩下的糖果中能满足他的最小的糖果，这样得到的分配方案，也就是满足的孩子个数最多的方案。**

  

- #### 钱币找零

  这个问题在我们的日常生活中更加普遍。

  假设我们有 **1 元、2 元、5 元、10 元、20 元、50 元、100 元**这些面额的纸币，它们的**张数分别是 c1、c2、c5、c10、c20、c50、c100**。

  我们现在要用这些钱来支付 K 元，最少要用多少张纸币呢？在生活中，我们肯定是先用面值最大的来支付，如果不够，就继续用更小一点面值的，以此类推，最后剩下的用 1 元来补齐。**在贡献相同期望值（纸币数目）的情况下，我们希望多贡献点金额，这样就可以让纸币数更少，这就是一种贪心算法的解决思路。**直觉告诉我们，这种处理方法就是最好的。实际上，要严谨地证明这种贪心算法的正确性，需要比较复杂的、有技巧的数学推导，我不建议你花太多时间在上面，不过如果感兴趣的话，可以自己去研究下。

  

- #### 区间覆盖

  假设我们有 n 个区间，区间的起始端点和结束端点分别是[l1, r1]，[l2, r2]，[l3, r3]，……，[ln, rn]。**我们从这 n 个区间中选出一部分区间，这部分区间满足两两不相交（端点相交的情况不算相交），最多能选出多少个区间呢？**

  <img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\区间.webp" style="zoom:50%;" />

  ​		这个问题的处理思路稍微不是那么好懂，不过，我建议你最好能弄懂，因为这个处理思想在很多贪心算法问题中都有用到，比如任务调度、教师排课等等问题。这个问题的解决思路是这样的：**我们假设这 n 个区间中最左端点是 `lmin`，最右端点是 `rmax`。**这个问题就相当于，我们选择几个不相交的区间，从左到右将`[lmin, rmax]`覆盖上。我们按照起始端点从小到大的顺序对这 n 个区间排序。我们每次选择的时候，左端点跟前面的已经覆盖的区间不重合的，右端点又尽量小的，这样可以让剩下的未覆盖区间尽可能的大，就可以放置更多的区间。这实际上就是一种贪心的选择方法。

  

### 用贪心算法实现霍夫曼编码

​		假设我有一个包含 1000 个字符的文件，每个字符占 1 个 byte（1byte=8bits），存储这 1000 个字符就一共需要 8000bits，那有没有更加节省空间的存储方式呢？

​		假设我们通过统计分析发现，这 1000 个字符中**只包含 6 种不同字符**，假设它们分别是 a、b、c、d、e、f。**而 3 个二进制位（bit）就可以表示 8 个不同的字符，所以，为了尽量减少存储空间，每个字符我们用 3 个二进制位来表示。**那存储这 1000 个字符只需要 3000bits 就可以了，比原来的存储方式节省了很多空间。**不过，还有没有更加节省空间的存储方式呢？**	

```go
a(000)、b(001)、c(010)、d(011)、e(100)、f(101)
```

​		霍夫曼编码就要登场了。**霍夫曼编码是一种十分有效的编码方法，广泛用于数据压缩中，其压缩率通常在 20%～90% 之间。**

​		霍夫曼编码不仅会考察**文本中有多少个不同字符，还会考察每个字符出现的频率，根据频率的不同，选择不同长度的编码**。霍夫曼编码试图用这种不等长的编码方法，来进一步增加压缩的效率。如何给不同频率的字符选择不同长度的编码呢？

​		根据贪心的思想，**我们可以把出现频率比较多的字符，用稍微短一些的编码；出现频率比较少的字符，用稍微长一些的编码。对于等长的编码来说，我们解压缩起来很简单。**比如刚才那个例子中，我们用 3 个 bit 表示一个字符。在解压缩的时候，我们每次从文本中读取 3 位二进制码，然后翻译成对应的字符。但是，霍夫曼编码是不等长的，每次应该读取 1 位还是 2 位、3 位等等来解压缩呢？这个问题就导致霍夫曼编码解压缩起来比较复杂。为了避免解压缩过程中的歧义，霍夫曼编码要求各个字符的编码之间，不会出现某个编码是另一个编码前缀的情况。

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\霍夫曼编码.webp" style="zoom:50%;" />

假设这 6 个字符出现的频率从高到低依次是 a、b、c、d、e、f。我们把它们编码下面这个样子，**任何一个字符的编码都不是另一个的前缀，在解压缩的时候，我们每次会读取尽可能长的可解压的二进制串，所以在解压缩的时候也不会歧义。经过这种编码压缩之后，这 1000 个字符只需要 2100bits 就可以了。**

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\pic.webp" style="zoom:50%;" />

尽管霍夫曼编码的思想并不难理解，但是如何根据字符出现频率的不同，给不同的字符进行不同长度的编码呢？这里的处理稍微有些技巧。我们把每个字符看作一个节点，并且附带着把频率放到优先级队列中。我们从队列中取出频率最小的两个节点 A、B，然后新建一个节点 C，把频率设置为两个节点的频率之和，并把这个新节点 C 作为节点 A、B 的父节点。最后再把 C 节点放入到优先级队列中。重复这个过程，直到队列中没有数据。

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\编码过程.webp" style="zoom:50%;" />

现在，我们给每一条边加上画一个权值，指向左子节点的边我们统统标记为 0，指向右子节点的边，我们统统标记为 1，那从根节点到叶节点的路径就是叶节点对应字符的霍夫曼编码。

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\编码结果.webp" style="zoom:50%;" />







## 10. 分治算法

分治算法（divide and conquer）的核心思想其实就是四个字，分而治之 ，**也就是将原问题划分成 n 个规模较小，并且结构与原问题相似的子问题，递归地解决这些子问题，然后再合并其结果，就得到原问题的解。**

这个定义看起来有点类似递归的定义。关于分治和递归的区别，我们在排序（下）的时候讲过，分治算法是一种处理问题的思想，递归是一种编程技巧。实际上，分治算法一般都比较适合用递归来实现。分治算法的递归实现中，每一层递归都会涉及这样三个操作：

- 分解：将原问题分解成一系列子问题；
- 解决：递归地求解各个子问题，若子问题足够小，则直接求解；
- 合并：将子问题的结果合并成原问题。

**分治算法能解决的问题，一般需要满足下面这几个条件：**

- 原问题与分解成的小问题具有相同的模式；
- 原问题分解成的子问题可以独立求解，子问题之间没有相关性，这一点是分治算法跟动态规划的明显区别，等我们讲到动态规划的时候，会详细对比这两种算法；
- 具有分解终止条件，也就是说，当问题足够小时，可以直接求解；
- 可以将子问题合并成原问题，而这个合并操作的复杂度不能太高，否则就起不到减小算法总体复杂度的效果了。



### 分治算法应用举例

我们用有序度来表示一组数据的有序程度，用逆序度表示一组数据的无序程度。

假设我们有 n 个数据，**我们期望数据从小到大排列，那完全有序的数据的有序度就是 n(n-1)/2，逆序度等于 0；相反，倒序排列的数据的有序度就是 0，逆序度是 n(n-1)/2**。除了这两种极端情况外，我们通过计算有序对或者逆序对的个数，来表示数据的有序度或逆序度。

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\逆序度.webp" style="zoom:50%;" />

我现在的问题是，**如何编程求出一组数据的有序对个数或者逆序对个数呢？**

​		因为有序对个数和逆序对个数的求解方式是类似的，所以你可以只思考逆序对个数的求解方法。

​		最笨的方法是，拿每个数字跟它后面的数字比较，看有几个比它小的。我们把比它小的数字个数记作 k，通过这样的方式，把每个数字都考察一遍之后，然后对每个数字对应的 k 值求和，最后得到的总和就是逆序对个数。不过，这样操作的时间复杂度是 O(n^2)。

那有没有更加高效的处理方法呢？

**我们用分治算法来试试。我们套用分治的思想来求数组 A 的逆序对个数。**

​		我们可以将数组**分成前后两半 A1 和 A2，分别计算 A1 和 A2 的逆序对个数 K1 和 K2，然后再计算 A1 与 A2 之间的逆序对个数 K3。那数组 A 的逆序对个数就等于 K1+K2+K3。**

​		我们前面讲过，使用分治算法其中一个要求是，子问题合并的代价不能太大，否则就起不了降低时间复杂度的效果了。那回到这个问题，如何快速计算出两个子问题 A1 与 A2 之间的逆序对个数呢？

​		这里就要借助归并排序算法了。你可以先试着想想，如何借助归并排序算法来解决呢？归并排序中有一个非常关键的操作，就是将两个有序的小数组，合并成一个有序的数组。**实际上，在这个合并的过程中，我们就可以计算这两个小数组的逆序对个数了。每次合并操作，我们都计算逆序对个数，把这些计算出来的逆序对个数求和，就是这个数组的逆序对个数了。**

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\统计个数.webp" style="zoom:50%;" />

```java

private int num = 0; // 全局变量或者成员变量

public int count(int[] a, int n) {
  num = 0;
  mergeSortCounting(a, 0, n-1);
  return num;
}

private void mergeSortCounting(int[] a, int p, int r) {
  if (p >= r) return;
  int q = (p+r)/2;
  mergeSortCounting(a, p, q);
  mergeSortCounting(a, q+1, r);
  merge(a, p, q, r);
}

private void merge(int[] a, int p, int q, int r) {
  int i = p, j = q+1, k = 0;
  int[] tmp = new int[r-p+1];
  while (i<=q && j<=r) {
    if (a[i] <= a[j]) {
      tmp[k++] = a[i++];
    } else {
      num += (q-i+1); // 统计p-q之间，比a[j]大的元素个数
      tmp[k++] = a[j++];
    }
  }
  while (i <= q) { // 处理剩下的
    tmp[k++] = a[i++];
  }
  while (j <= r) { // 处理剩下的
    tmp[k++] = a[j++];
  }
  for (i = 0; i <= r-p; ++i) { // 从tmp拷贝回a
    a[p+i] = tmp[i];
  }
}
```



### 在海量数据处理中的应用

比如，给 10GB 的订单文件按照金额排序这样一个需求，看似是一个简单的排序问题，但是因为数据量大，有 10GB，而我们的机器的内存可能只有 2、3GB 这样子，无法一次性加载到内存，也就无法通过单纯地使用快排、归并等基础算法来解决了。

​		给 10GB 的订单排序，我们就可以先扫描一遍订单，根据订单的金额，将 10GB 的文件划分为几个金额区间。比如订单金额为 1 到 100 元的放到一个小文件，101 到 200 之间的放到另一个文件，以此类推。这样每个小文件都可以单独加载到内存排序，最后将这些有序的小文件合并，就是最终有序的 10GB 订单数据了。

​		如果订单数据存储在类似 GFS 这样的分布式系统上，当 10GB 的订单被划分成多个小文件的时候，每个文件可以并行加载到多台机器上处理，最后再将结果合并在一起，这样并行处理的速度也加快了很多。**不过，这里有一个点要注意，就是数据的存储与计算所在的机器是同一个或者在网络中靠的很近（比如一个局域网内，数据存取速度很快），否则就会因为数据访问的速度，导致整个处理过程不但不会变快，反而有可能变慢。**

​		实际上，**MapReduce 框架只是一个任务调度器，底层依赖 GFS 来存储数据，依赖 Borg 管理机器**。它从 GFS 中拿数据，交给 Borg 中的机器执行，并且时刻监控机器执行的进度，一旦出现机器宕机、进度卡壳等，就重新从 Borg 中调度一台机器执行。尽管 MapReduce 的模型非常简单，但是在 Google 内部应用非常广泛。它除了可以用来处理这种数据与数据之间存在关系的任务，比如 MapReduce 的经典例子，统计文件中单词出现的频率。除此之外，它还可以用来处理数据与数据之间没有关系的任务，比如对网页分析、分词等，每个网页可以独立的分析、分词，而这两个网页之间并没有关系。网页几十亿、上百亿，如果单机处理，效率低下，我们就可以利用 MapReduce 提供的高可靠、高性能、高容错的并行计算框架，并行地处理这几十亿、上百亿的网页。内容小结



## 11. 回溯算法

回溯的处理思想，有点类似枚举搜索。我们枚举所有的解，找到满足期望的解。为了有规律地枚举所有可能的解，避免遗漏和重复，我们把问题求解的过程分为多个阶段。每个阶段，我们都会面对一个岔路口，我们先随意选一条路走，当发现这条路走不通的时候（不符合期望的解），就回退到上一个岔路口，另选一种走法继续走。

- #### 八皇后问题

  我们有一个 8x8 的棋盘，希望往里放 8 个棋子（皇后），每个棋子所在的行、列、对角线都不能有另一个棋子。你可以看我画的图，第一幅图是满足条件的一种方法，第二幅图是不满足条件的。八皇后问题就是期望找到所有满足这种要求的放棋子方式。

  <img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\皇后.webp" style="zoom:50%;" />

  ​		我们把这个问题划分成 8 个阶段，依次将 8 个棋子放到第一行、第二行、第三行……第八行。在放置的过程中，我们不停地检查当前放法，是否满足要求。如果满足，则跳到下一行继续放置棋子；如果不满足，那就再换一种放法，继续尝试。

  ​		回溯算法非常适合用递归代码实现，所以，我把八皇后的算法翻译成代码。我在代码里添加了详细的注释，你可以对比着看下。如果你之前没有接触过八皇后问题，建议你自己用熟悉的编程语言实现一遍，这对你理解回溯思想非常有帮助。

  ```java
  
  int[] result = new int[8];//全局或成员变量,下标表示行,值表示queen存储在哪一列
  public void cal8queens(int row) { // 调用方式：cal8queens(0);
    if (row == 8) { // 8个棋子都放置好了，打印结果
      printQueens(result);
      return; // 8行棋子都放好了，已经没法再往下递归了，所以就return
    }
    for (int column = 0; column < 8; ++column) { // 每一行都有8中放法
      if (isOk(row, column)) { // 有些放法不满足要求
        result[row] = column; // 第row行的棋子放到了column列
        cal8queens(row+1); // 考察下一行
      }
    }
  }
  
  private boolean isOk(int row, int column) {//判断row行column列放置是否合适
    int leftup = column - 1, rightup = column + 1;
    for (int i = row-1; i >= 0; --i) { // 逐行往上考察每一行
      if (result[i] == column) return false; // 第i行的column列有棋子吗？
      if (leftup >= 0) { // 考察左上对角线：第i行leftup列有棋子吗？
        if (result[i] == leftup) return false;
      }
      if (rightup < 8) { // 考察右上对角线：第i行rightup列有棋子吗？
        if (result[i] == rightup) return false;
      }
      --leftup; ++rightup;
    }
    return true;
  }
  
  private void printQueens(int[] result) { // 打印出一个二维矩阵
    for (int row = 0; row < 8; ++row) {
      for (int column = 0; column < 8; ++column) {
        if (result[row] == column) System.out.print("Q ");
        else System.out.print("* ");
      }
      System.out.println();
    }
    System.out.println();
  }
  ```



### 两个回溯算法的经典应用

#### 1. 0-1 背包

​		0-1 背包是非常经典的算法问题，很多场景都可以抽象成这个问题模型。

​		这个问题的经典解法是动态规划，不过还有一种简单但没有那么高效的解法，那就是今天讲的**回溯算法**。

​		动态规划的解法我下一节再讲，我们先来看下，如何用回溯法解决这个问题。0-1 背包问题有很多变体，我这里介绍一种比较基础的。

​		我们有一个背包，**背包总的承载重量是 Wkg。现在我们有 n 个物品，每个物品的重量不等，并且不可分割**。

​		我们现在期望选择几件物品，装载到背包中。**在不超过背包所能装载重量的前提下，如何让背包中物品的总重量最大？**

​		实际上，背包问题我们在贪心算法那一节，已经讲过一个了，不过那里讲的物品是可以分割的，我可以装某个物品的一部分到背包里面。今天讲的这个背包问题，物品是不可分割的，要么装要么不装，所以叫 0-1 背包问题。显然，这个问题已经无法通过贪心算法来解决了。

​		我们现在来看看，用回溯算法如何来解决。**对于每个物品来说，都有两种选择，装进背包或者不装进背包。对于 n 个物品来说，总的装法就有 2^n 种，去掉总重量超过 Wkg 的，从剩下的装法中选择总重量最接近 Wkg 的。**

​		不过，我们如何才能不重复地穷举出这 2^n 种装法呢？这里就可以用回溯的方法。**我们可以把物品依次排列，整个问题就分解为了 n 个阶段**，每个阶段对应一个物品怎么选择。先对第一个物品进行处理，选择装进去或者不装进去，然后再递归地处理剩下的物品。描述起来很费劲，我们直接看代码，反而会更加清晰一些。这里还稍微用到了一点搜索剪枝的技巧，就是当发现已经选择的物品的重量超过 Wkg 之后，我们就停止继续探测剩下的物品。你可以看我写的具体的代码。

```java

public int maxW = Integer.MIN_VALUE; //存储背包中物品总重量的最大值
// cw表示当前已经装进去的物品的重量和；i表示考察到哪个物品了；
// w背包重量；items表示每个物品的重量；n表示物品个数
// 假设背包可承受重量100，物品个数10，物品重量存储在数组a中，那可以这样调用函数：
// f(0, 0, a, 10, 100)
public void f(int i, int cw, int[] items, int n, int w) {
  if (cw == w || i == n) { // cw==w表示装满了;i==n表示已经考察完所有的物品
    if (cw > maxW) maxW = cw;
    return;
  }
  f(i+1, cw, items, n, w);
  if (cw + items[i] <= w) {// 已经超过可以背包承受的重量的时候，就不要再装了
    f(i+1,cw + items[i], items, n, w);
  }
}
```



#### 2. 正则表达式

看懂了 0-1 背包问题，我们再来看另外一个例子，正则表达式匹配。

​		对于一个开发工程师来说，正则表达式你应该不陌生吧？在平时的开发中，或多或少都应该用过。

​		实际上，正则表达式里最重要的一种算法思想就是**回溯**。正则表达式中，最重要的就是通配符，通配符结合在一起，可以表达非常丰富的语义。

​		为了方便讲解，我假设正则表达式中只包含**“*”**和**“?”**这两种通配符，并且对这两个通配符的语义稍微做些改变。

​		其中，**“*”**匹配任意多个（大于等于 0 个）任意字符，“?”匹配零个或者一个任意字符。基于以上背景假设，我们看下，如何用回溯算法，判断一个给定的文本，能否跟给定的正则表达式匹配？

​		我们依次考察正则表达式中的每个字符，**当是非通配符时，我们就直接跟文本的字符进行匹配，如果相同，则继续往下处理；如果不同，则回溯。**

​		如果遇到特殊字符的时候，我们就有多种处理方式了，也就是所谓的岔路口，**比如“*”有多种匹配方案，可以匹配任意个文本串中的字符，我们就先随意的选择一种匹配方案，然后继续考察剩下的字符。如果中途发现无法继续匹配下去了，我们就回到这个岔路口，重新选择一种匹配方案，然后再继续匹配剩下的字符。**有了前面的基础，是不是这个问题就好懂多了呢？我把这个过程翻译成了代码，你可以结合着一块看下，应该有助于你理解。

```java

public class Pattern {
  private boolean matched = false;
  private char[] pattern; // 正则表达式
  private int plen; // 正则表达式长度

  public Pattern(char[] pattern, int plen) {
    this.pattern = pattern;
    this.plen = plen;
  }

  public boolean match(char[] text, int tlen) { // 文本串及长度
    matched = false;
    rmatch(0, 0, text, tlen);
    return matched;
  }

  private void rmatch(int ti, int pj, char[] text, int tlen) {
    if (matched) return; // 如果已经匹配了，就不要继续递归了
    if (pj == plen) { // 正则表达式到结尾了
      if (ti == tlen) matched = true; // 文本串也到结尾了
      return;
    }
    if (pattern[pj] == '*') { // *匹配任意个字符
      for (int k = 0; k <= tlen-ti; ++k) {
        rmatch(ti+k, pj+1, text, tlen);
      }
    } else if (pattern[pj] == '?') { // ?匹配0个或者1个字符
      rmatch(ti, pj+1, text, tlen);
      rmatch(ti+1, pj+1, text, tlen);
    } else if (ti < tlen && pattern[pj] == text[ti]) { // 纯字符匹配才行
      rmatch(ti+1, pj+1, text, tlen);
    }
  }
}
```



## 12. 动态规划

我在讲贪心算法、回溯算法的时候，多次讲到**背包问题。**

### 背包问题

​		今天，我们依旧拿这个问题来举例。对于一组不同重量、不可分割的物品，我们需要选择一些装入背包，在满足背包最大重量限制的前提下，背包中物品总重量的最大值是多少呢？

​		关于这个问题，我们上一节讲了回溯的解决方法，也就是穷举搜索所有可能的装法，然后找出满足条件的最大值。不过，回溯算法的复杂度比较高，是指数级别的。那有没有什么规律，可以有效降低时间复杂度呢？我们一起来看看。

```java
// 回溯算法实现。注意：我把输入的变量都定义成了成员变量。
private int maxW = Integer.MIN_VALUE; // 结果放到maxW中
private int[] weight = {2，2，4，6，3};  // 物品重量
private int n = 5; // 物品个数
private int w = 9; // 背包承受的最大重量
public void f(int i, int cw) { // 调用f(0, 0)
  if (cw == w || i == n) { // cw==w表示装满了，i==n表示物品都考察完了
    if (cw > maxW) maxW = cw;
    return;
  }
  f(i+1, cw); // 选择不装第i个物品
  if (cw + weight[i] <= w) {
    f(i+1,cw + weight[i]); // 选择装第i个物品
  }
}
```

```go
var weight = []int{2, 2, 4, 6, 3} // 物品重量
var n = 5                         // 物品个数
var w int = 9                     // 背包承受的最大重量
func zeroOneBeg(i int, cw int) { 		// 调用f(0, 0)
	if cw == w || i == n { // cw==w表示装满了，i==n表示物品都考察完了
		if cw > maxW {
			maxW = cw
		}
		return
	}
	f(i+1, cw) // 选择不装第i个物品
	if cw+weight[i] <= w {
		f(i+1, cw+weight[i]) // 选择装第i个物品
	}
}
```

规律是不是不好找？那我们就举个例子、画个图看看。我们假设背包的最大承载重量是 9。我们有 5 个不同的物品，每个物品的重量分别是 2，2，4，6，3。

​		如果我们把这个例子的回溯求解过程，用递归树画出来，就是下面这个样子

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\递归树1.webp" style="zoom:50%;" />

递归树中的每个节点表示一种状态，我们用**`（i, cw）`**来表示。其中，**`i`** 表示将要决策第几个物品是否装入背包，**`cw`** 表示当前背包中物品的总重量。比如，（2，2）表示我们将要决策第 2 个物品是否装入背包，在决策前，背包中物品的总重量是 2。

从递归树中，你应该能会发现，有些子问题的求解是重复的，比如图中 **f(2, 2)** 和 **f(3,4)** 都被重复计算了两次。我们可以借助递归那一节讲的“备忘录”的解决方式，记录已经计算好的 **`f(i, cw)`**，当再次计算到重复的 **`f(i, cw)`** 的时候，可以直接从备忘录中取出来用，就不用再递归计算了，这样就可以避免冗余计算。

```java
private int maxW = Integer.MIN_VALUE; // 结果放到maxW中
private int[] weight = {2，2，4，6，3};  // 物品重量
private int n = 5; // 物品个数
private int w = 9; // 背包承受的最大重量
private boolean[][] mem = new boolean[5][10]; // 备忘录，默认值false
public void f(int i, int cw) { // 调用f(0, 0)
  if (cw == w || i == n) { // cw==w表示装满了，i==n表示物品都考察完了
    if (cw > maxW) maxW = cw;
    return;
  }
  if (mem[i][cw]) return; // 重复状态
  mem[i][cw] = true; // 记录(i, cw)这个状态
  f(i+1, cw); // 选择不装第i个物品
  if (cw + weight[i] <= w) {
    f(i+1,cw + weight[i]); // 选择装第i个物品
  }
}
```

**动态规划的思路：**

我们把整个求解过程分为 n 个阶段，每个阶段会决策一个物品是否放到背包中。

​		**每个物品决策（放入或者不放入背包）完之后，背包中的物品的重量会有多种情况，也就是说，会达到多种不同的状态，对应到递归树中，就是有很多不同的节点。**

​		我们把每一层重复的状态（节点）合并，只记录不同的状态，然后基于上一层的状态集合，来推导下一层的状态集合。

​		我们可以通过合并每一层重复的状态，这样就保证**每一层不同状态的个数都不会超过 w 个（w 表示背包的承载重量）**，也就是例子中的 9。于是，我们就成功避免了每层状态个数的指数级增长。

​		我们用一个二维数组 **`states[n][w+1]`**，来记录每层可以达到的不同状态。第 0 个（下标从 0 开始编号）物品的重量是 2，要么装入背包，要么不装入背包，决策完之后，会对应背包的两种状态，背包中物品的总重量是 0 或者 2。我们用 **`states[0][0]=true`** 和 **`states[0][2]=true`** 来表示这两种状态。

​		第 1 个物品的重量也是 2，基于之前的背包状态，在这个物品决策完之后，不同的状态有 3 个，背包中物品总重量分别是 0(0+0)，2(0+2 or 2+0)，4(2+2)。我们用 **`states[1][0]=true`**，**`states[1][2]=true`**，**`states[1][4]=true`** 来表示这三种状态。以此类推，直到考察完所有的物品后，整个 states 状态数组就都计算好了。

​		我把整个计算的过程画了出来，你可以看看。图中 0 表示 false，1 表示 true。我们只需要在最后一层，找一个值为 true 的最接近 w（这里是 9）的值，就是背包中物品总重量的最大值。

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\final.webp" style="zoom:50%;" />

文字描述可能还不够清楚。我把上面的过程，翻译成代码，你可以结合着一块看下

```java
//weight:物品重量，n:物品个数，w:背包可承载重量
public int knapsack(int[] weight, int n, int w) {
  boolean[][] states = new boolean[n][w+1]; // 默认值false
  states[0][0] = true;  // 第一行的数据要特殊处理，可以利用哨兵优化
  if (weight[0] <= w) {
    states[0][weight[0]] = true;
  }
  for (int i = 1; i < n; ++i) { // 动态规划状态转移
    for (int j = 0; j <= w; ++j) {// 不把第i个物品放入背包
      if (states[i-1][j] == true) states[i][j] = states[i-1][j];
    }
    for (int j = 0; j <= w-weight[i]; ++j) {//把第i个物品放入背包
      if (states[i-1][j]==true) states[i][j+weight[i]] = true;
    }
  }
  for (int i = w; i >= 0; --i) { // 输出结果
    if (states[n-1][i] == true) return i;
  }
  return 0;
}
```

```go

func knapsack(weight []int, n, w int) int {
	var states [][]bool
	states[0][0] = true
	if weight[0] <= w {
		states[0][weight[0]] = true
	}
	for i := 1; i < n; i++ {
		for j := 0; j <= w; j++ {
			// 不把第i个物品放入背包
			if states[i-1][j] == true {
				states[i][j] = states[i-1][j]
			}
		}
		for j := 0; j <= w-weight[i] ; j++ {
			// 将第i个物品放入背包中
			if states[i-1][j] == true{
				states[i][j+weight[j]] = true
			}
		}
	}
	for i := w; i >=0 ; i-- {
		// 输出结果
		if states[n-1][i] == true{
			return i
		}
	}
	return 0
}

```

**实际上，这就是一种用动态规划解决问题的思路。**

​		我们把问题分解为多个阶段，每个阶段对应一个决策。**我们记录每一个阶段可达的状态集合（去掉重复的），然后通过当前阶段的状态集合，来推导下一个阶段的状态集合，动态地往前推进。**

​		这也是动态规划这个名字的由来，你可以自己体会一下，是不是还挺形象的？前面我们讲到，用回溯算法解决这个问题的时间复杂度 O(2^n)，是指数级的。那动态规划解决方案的时间复杂度是多少呢？

​		这个代码的时间复杂度非常好分析，耗时最多的部分就是代码中的两层 for 循环，所以时间复杂度是 **`O(n*w)`**。n 表示物品个数，w 表示背包可以承载的总重量。

​		从理论上讲，指数级的时间复杂度肯定要比 **`O(n*w)`** 高很多，但是为了让你有更加深刻的感受，我来举一个例子给你比较一下。

​		我们假设有 10000 个物品，重量分布在 1 到 15000 之间，背包可以承载的总重量是 30000。如果我们用回溯算法解决，用具体的数值表示出时间复杂度，就是 2^10000，这是一个相当大的一个数字。

​		如果我们用动态规划解决，用具体的数值表示出时间复杂度，就是 10000*30000。虽然看起来也很大，但是和 2^10000 比起来，要小太多了。

​		尽管动态规划的执行效率比较高，但是就刚刚的代码实现来说，我们需要额外申请一个 n 乘以 w+1 的二维数组，对空间的消耗比较多。**所以，有时候，我们会说，动态规划是一种空间换时间的解决思路。**

​		你可能要问了，有什么办法可以降低空间消耗吗？实际上，我们只需要一个大小为 w+1 的一维数组就可以解决这个问题。动态规划状态转移的过程，都可以基于这个一维数组来操作。具体的代码实现我贴在这里，你可以仔细看下。

```java
public static int knapsack2(int[] items, int n, int w) {
  boolean[] states = new boolean[w+1]; // 默认值false
  states[0] = true;  // 第一行的数据要特殊处理，可以利用哨兵优化
  if (items[0] <= w) {
    states[items[0]] = true;
  }
  for (int i = 1; i < n; ++i) { // 动态规划
    for (int j = w-items[i]; j >= 0; --j) {//把第i个物品放入背包
      if (states[j]==true) states[j+items[i]] = true;
    }
  }
  for (int i = w; i >= 0; --i) { // 输出结果
    if (states[i] == true) return i;
  }
  return 0;
}
```

```go
func knapsack2(items []int, n, w int) int {
	states := make([]bool, w+1) // 默认值false
	states[0] = true            // 第一行的数据要特殊处理，可以利用哨兵优化
	if items[0] <= w {
		states[0] = true
	}
	for i := 1; i < n; i++ {
		for j := w - items[i]; j >= 0; j-- {
			if states[j] == true {
				states[j+items[i]] = true
			}
		}
	}
	for i := w; i >= 0; i-- {
		if states[i] == true {
			return i
		}
	}
	return 0
}
```

​		代码：遍历每个物品，将该物品放入背包时，在不超过最大重量的前提下，再遍历查看之前的放入记录，将之前可能出现的重量的和当前物品的重量相加再记录下来，等所有方案都尝试过后，可能出现的背包重量也都被记录下来了，最后，从中选择一个最大值返回；

​		 j从大到小是因为：如果从小到大，j=0，items[i] =2时，states[j+items[i]]肯定会赋值为true，也就是states[2] = true; 当遍历到j=2时，因为states[j] == true, 则states[4]就会被赋值为true; 同样的当j=4,6时，states[6], states[8]也会被赋值为true；这就相当于item[i]这个物品被使用了多次



#### **0-1 背包问题升级版**

​		我们刚刚讲的背包问题，只涉及背包重量和物品重量。我们现在引入物品价值这一变量。对于一组不同重量、不同价值、不可分割的物品，我们选择将某些物品装入背包，在满足背包最大重量限制的前提下，背包中可装入物品的总价值最大是多少呢？这个问题依旧可以用回溯算法来解决。这个问题并不复杂，所以具体的实现思路，我就不用文字描述了，直接给你看代码。

```java
private int maxV = Integer.MIN_VALUE; // 结果放到maxV中
private int[] items = {2，2，4，6，3};  // 物品的重量
private int[] value = {3，4，8，9，6}; // 物品的价值
private int n = 5; // 物品个数
private int w = 9; // 背包承受的最大重量
public void f(int i, int cw, int cv) { // 调用f(0, 0, 0)
  if (cw == w || i == n) { // cw==w表示装满了，i==n表示物品都考察完了
    if (cv > maxV) maxV = cv;
    return;
  }
  f(i+1, cw, cv); // 选择不装第i个物品
  if (cw + weight[i] <= w) {
    f(i+1,cw+weight[i], cv+value[i]); // 选择装第i个物品
  }
}
```

针对上面的代码，我们还是照例画出递归树。在递归树中，每个节点表示一个状态。现在我们需要 3 个变量**`（i, cw, cv）`**来表示一个状态。其中，**`i`** 表示即将要决策第 **`i`** 个物品是否装入背包，**`cw`** 表示当前背包中物品的总重量，**`cv`** 表示当前背包中物品的总价值。

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\cvcw.webp" style="zoom:50%;" />

我们发现，在递归树中，有几个节点的 **`i`** 和 **`cw`** 是完全相同的，比如 f(2,2,4) 和 f(2,2,3)。

​		在背包中物品总重量一样的情况下，f(2,2,4) 这种状态对应的物品总价值更大，我们可以舍弃 f(2,2,3) 这种状态，只需要沿着 f(2,2,4) 这条决策路线继续往下决策就可以。		

​		也就是说，对于 **`(i, cw)`** 相同的不同状态，那我们只需要保留 cv 值最大的那个，继续递归处理，其他状态不予考虑。思路说完了，但是代码如何实现呢？如果用回溯算法，这个问题就没法再用“备忘录”解决了。

​		所以，我们就需要换一种思路，看看动态规划是不是更容易解决这个问题？我们还是把整个求解过程分为 n 个阶段，每个阶段会决策一个物品是否放到背包中。每个阶段决策完之后，背包中的物品的总重量以及总价值，会有多种情况，也就是会达到多种不同的状态。我们用一个二维数组 **`states[n][w+1]`**，来记录每层可以达到的不同状态。不过这里数组存储的值不再是 **`boolean`** 类型的了，而是当前状态对应的最大总价值。我们把每一层中 **`(i, cw)`** 重复的状态（节点）合并，只记录 cv 值最大的那个状态，然后基于这些状态来推导下一层的状态。我们把这个动态规划的过程翻译成代码，就是下面这个样子：

```java

public static int knapsack3(int[] weight, int[] value, int n, int w) {
  int[][] states = new int[n][w+1];
  for (int i = 0; i < n; ++i) { // 初始化states
    for (int j = 0; j < w+1; ++j) {
      states[i][j] = -1;
    }
  }
  states[0][0] = 0;
  if (weight[0] <= w) {
    states[0][weight[0]] = value[0];
  }
  for (int i = 1; i < n; ++i) { //动态规划，状态转移
    for (int j = 0; j <= w; ++j) { // 不选择第i个物品
      if (states[i-1][j] >= 0) states[i][j] = states[i-1][j];
    }
    for (int j = 0; j <= w-weight[i]; ++j) { // 选择第i个物品
      if (states[i-1][j] >= 0) {
        int v = states[i-1][j] + value[i];
        if (v > states[i][j+weight[i]]) {
          states[i][j+weight[i]] = v;
        }
      }
    }
  }
  // 找出最大值
  int maxvalue = -1;
  for (int j = 0; j <= w; ++j) {
    if (states[n-1][j] > maxvalue) maxvalue = states[n-1][j];
  }
  return maxvalue;
}
```

### “一个模型三个特征”理论讲解

首先，我们来看，什么是**“一个模型”？**它指的是动态规划适合解决的问题的模型。

​		我把这个模型定义为**“多阶段决策最优解模型”**。下面我具体来给你讲讲。我们一般是用动态规划来解决最优问题。而解决问题的过程，需要经历多个决策阶段。每个决策阶段都对应着一组状态。然后我们寻找一组决策序列，经过这组决策序列，能够产生最终期望求解的最优值。现在，我们再来看，什么是“三个特征”？它们分别是**最优子结构、无后效性和重复子问题**。这三个概念比较抽象，我来逐一详细解释一下。



- #### 最优子结构

  ​        最优子结构指的是，问题的最优解包含子问题的最优解。反过来说就是，我们可以通过子问题的最优解，推导出问题的最优解。如果我们把最优子结构，对应到我们前面定义的动态规划问题模型上，那我们也可以理解为，**后面阶段的状态可以通过前面阶段的状态推导出来**。

- #### 无后效性

  ​        无后效性有两层含义，第一层含义是，在推导后面阶段的状态的时候，**我们只关心前面阶段的状态值，不关心这个状态是怎么一步一步推导出来的**。第二层含义是，**某阶段状态一旦确定，就不受之后阶段的决策影响**。无后效性是一个非常“宽松”的要求。只要满足前面提到的动态规划问题模型，其实基本上都会满足无后效性。

- #### 重复子问题

  ​        这个概念比较好理解。前面一节，我已经多次提过。如果用一句话概括一下，那就是，**不同的决策序列，到达某个相同的阶段时，可能会产生重复的状态。**

  

### “一个模型三个特征”实例剖析

​		假设我们有一个 n 乘以 n 的矩阵 **`w[n][n]`**。矩阵存储的都是正整数。棋子起始位置在左上角，终止位置在右下角。我们将棋子从左上角移动到右下角。**每次只能向右或者向下移动一位。**从左上角到右下角，会有很多不同的路径可以走。我们把每条路径经过的数字加起来看作路径的长度。那从左上角移动到右下角的最短路径长度是多少呢？

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\棋盘.webp" style="zoom:50%;" />

我们先看看，这个问题**是否符合“一个模型”**？

​		从 (0, 0) 走到 (n-1, n-1)，总共要走 **`2*(n-1)`** 步，也就对应着 **`2*(n-1)`** 个阶段。每个阶段都有向右走或者向下走两种决策，并且每个阶段都会对应一个状态集合。我们把状态定义为 **`min_dist(i, j)`**，其中**`i`**表示行，**`j`** 表示列。**`min_dist`** 表达式的值表示从 **`(0, 0)`** 到达 **`(i, j)`** 的最短路径长度。所以，这个问题是一个多阶段决策最优解问题，符合动态规划的模型。

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\各阶段.webp" style="zoom: 33%;" />

我们再来看，**这个问题是否符合“三个特征”**？

我们可以用回溯算法来解决这个问题。如果你自己写一下代码，画一下递归树，就会发现，递归树中有重复的节点。重复的节点表示，从左上角到节点对应的位置，有多种路线，这也能说明这个问题中存在重复子问题。

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\重复子问题.webp" style="zoom:50%;" />

​		如果我们走到 **`(i, j)`** 这个位置，我们只能通过 **`(i-1, j)，(i, j-1)`** 这两个位置移动过来，也就是说，我们想要计算 **`(i, j)`** 位置对应的状态，只需要关心 **`(i-1, j)，(i, j-1)`** 两个位置对应的状态，并不关心棋子是通过什么样的路线到达这两个位置的。

​		而且，我们仅仅允许往下和往右移动，不允许后退，所以，**前面阶段的状态确定之后，不会被后面阶段的决策所改变，所以，这个问题符合“无后效性”这一特征**。

​		刚刚定义状态的时候，我们把从起始位置 **`(0, 0)`** 到 **`(i, j)`** 的最小路径，记作 **`min_dist(i, j)`**。因为我们只能往右或往下移动，所以，我们只有可能从 **`(i, j-1)`** 或者 **`(i-1, j)`** 两个位置到达 **`(i, j)`**。也就是说，到达 **`(i, j)`** 的最短路径要么经过 **`(i, j-1)`**，要么经过 **`(i-1, j)`**，而且到达 **`(i, j)`** 的最短路径肯定包含到达这两个位置的最短路径之一。换句话说就是**`，min_dist(i, j)`** 可以通过 **`min_dist(i, j-1)`** 和 **`min_dist(i-1, j)`** 两个状态推导出来。这就说明，**这个问题符合“最优子结构”。**

​								**`	min_dist(i, j) = w[i][j] + min(min_dist(i, j-1), min_dist(i-1, j))`**



### 两种动态规划解题思路总结

刚刚我讲了，如何鉴别一个问题是否可以用动态规划来解决。

现在，我再总结一下，动态规划解题的一般思路，让你面对动态规划问题的时候，能够有章可循，不至于束手无策。我个人觉得，解决动态规划问题，一般有两种思路。我把它们分别叫作，**状态转移表法和状态转移方程法。**

- #### 状态转移表法

  一般能用动态规划解决的问题，都可以使用回溯算法的暴力搜索解决。

  所以，当我们拿到问题的时候，我们可以先用简单的回溯算法解决，然后定义状态，每个状态表示一个节点，**然后对应画出递归树**。

  从递归树中，我们很容易可以看出来，**是否存在重复子问题，以及重复子问题是如何产生的。以此来寻找规律，看是否能用动态规划解决。**找到重复子问题之后，接下来，我们有两种处理思路。

  - 第一种是直接用回溯加“备忘录”的方法，来避免重复子问题。从执行效率上来讲，这跟动态规划的解决思路没有差别。

  - 第二种是使用动态规划的解决方法，**状态转移表法**。第一种思路，我就不讲了，你可以看看上一节的两个例子。我们重点来看状态转移表法是如何工作的。

  **我们先画出一个状态表**。状态表一般都是二维的，所以你可以把它想象成二维数组。

  ​		其中，每个状态包含三个变量，**行、列、数组值**。我们根据决策的先后过程，从前往后，根据递推关系，分阶段填充状态表中的每个状态。最后，我们将这个递推填表的过程，翻译成代码，就是动态规划代码了。

  ​		尽管大部分状态表都是二维的，但是如果问题的状态比较复杂，需要很多变量来表示，**那对应的状态表可能就是高维的**，比如三维、四维。那这个时候，我们就不适合用状态转移表法来解决了。**一方面是因为高维状态转移表不好画图表示，另一方面是因为人脑确实很不擅长思考高维的东西**。

  ​		现在，我们来看一下，如何套用这个状态转移表法，来解决之前那个矩阵最短路径的问题？从起点到终点，我们有很多种不同的走法。我们可以穷举所有走法，然后对比找出一个最短走法。不过如何才能无重复又不遗漏地穷举出所有走法呢？我们可以用回溯算法这个比较有规律的穷举算法。回溯算法的代码实现如下所示。代码很短，而且我前面也分析过很多回溯算法的例题，这里我就不多做解释了，你自己来看看。

  ```java
  private int minDist = Integer.MAX_VALUE  // 全局变量或者成员变量
  // 调用方式：minDistBacktracing(0, 0, 0, w, n);
  public void minDistBT(int i, int j, int dist, int[][] w, int n) {
    // 到达了n-1, n-1这个位置了，这里看着有点奇怪哈，你自己举个例子看下
    if (i == n && j == n) {
      if (dist < minDist) minDist = dist;
      return;
    }
    if (i < n) { // 往下走，更新i=i+1, j=j
      minDistBT(i + 1, j, dist+w[i][j], w, n);
    }
    if (j < n) { // 往右走，更新i=i, j=j+1
      minDistBT(i, j+1, dist+w[i][j], w, n);
    }
  }
  ```

  有了回溯代码之后，接下来，我们要画出递归树，以此来寻找重复子问题。

  在递归树中，一个状态（也就是一个节点）包含三个变量 **`(i, j, dist)`**，其中 **`i，j`** 分别表示行和列，**`dist`** 表示从起点到达 **`(i, j)`** 的路径长度。从图中，我们看出，尽管 **`(i, j, dist)`** 不存在重复的，但是 **`(i, j)`** 重复的有很多。对于 **`(i, j)`** 重复的节点，我们只需要选择 **`dist`** 最小的节点，继续递归求解，其他节点就可以舍弃了。

  <img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\dist.webp" style="zoom:50%;" />

  既然存在重复子问题，我们就可以尝试看下，是否可以用动态规划来解决呢？

  我们画出一个二维状态表，**表中的行、列表示棋子所在的位置，表中的数值表示从起点到这个位置的最短路径**。我们按照决策过程，通过不断状态递推演进，将状态表填好。为了方便代码实现，我们按行来进行依次填充。

  <img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\表1.webp" style="zoom:50%;" />

  <img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\表2.webp" style="zoom:50%;" />

  弄懂了填表的过程，代码实现就简单多了。我们将上面的过程，翻译成代码，就是下面这个样子。结合着代码、图和文字描述，应该更容易理解我讲的内容。

  ```java
  public int minDistDP(int[][] matrix, int n) {
    int[][] states = new int[n][n];
    int sum = 0;
    for (int j = 0; j < n; ++j) { // 初始化states的第一行数据
      sum += matrix[0][j];
      states[0][j] = sum;
    }
    sum = 0;
    for (int i = 0; i < n; ++i) { // 初始化states的第一列数据
      sum += matrix[i][0];
      states[i][0] = sum;
    }
    for (int i = 1; i < n; ++i) {
      for (int j = 1; j < n; ++j) {
        states[i][j] = 
              matrix[i][j] + Math.min(states[i][j-1], states[i-1][j]);
      }
    }
    return states[n-1][n-1];
  }
  ```
  
  ```go
  func minDistDP(matrix [][]int, n int) int {
  	var states [][]int
  	var sum int
  	for j := 0; j < n; j++ { // 初始化states的第一行数据
  		sum += matrix[0][j]
  		states[0][j] = sum
  	}
  	sum = 0
  	for  i:= 0; i < n; i++{ // 初始化states的第一列数据
  		sum += matrix[i][0]
  		states[i][0] = sum
  	}
  	for i := 1; i < n; i++ {
  		for  j := 1; j < n;j++ {
  			states[i][j] =
  				matrix[i][j] + min(states[i][j-1], states[i-1][j]);
  		}
  	}
  	return states[n-1][n-1]
  }
  
  func min(i int, i2 int) int {
  	if i < i2{
  		return i
  	}else{
  		return i2
  	}
  }
  ```
  
  



- #### 状态转移方程法

  状态转移方程法有点类似递归的解题思路。

  我们需要分析，某个问题如何通过子问题来递归求解，也就是所谓的最优子结构。

  **根据最优子结构，写出递归公式，也就是所谓的状态转移方程。**有了状态转移方程，代码实现就非常简单了。

  ​		一般情况下，我们有两种代码实现方法，**一种是递归加“备忘录”，****另一种是迭代递推。**我们还是拿刚才的例子来举例。最优子结构前面已经分析过了，你可以回过头去再看下。为了方便你查看，我把状态转移方程放到这里。

  ```java
  min_dist(i, j) = w[i][j] + min(min_dist(i, j-1), min_dist(i-1, j))
  ```

  这里我强调一下，**状态转移方程是解决动态规划的关键**。如果我们能写出状态转移方程，那动态规划问题基本上就解决一大半了，而翻译成代码非常简单。但是很多动态规划问题的状态本身就不好定义，状态转移方程也就更不好想到。

  下面我用递归加“备忘录”的方式，将状态转移方程翻译成来代码，你可以看看。对于另一种实现方式，跟状态转移表法的代码实现是一样的，只是思路不同。

  ```java
  private int[][] matrix = 
           {{1，3，5，9}, {2，1，3，4}，{5，2，6，7}，{6，8，4，3}};
  private int n = 4;
  private int[][] mem = new int[4][4];
  public int minDist(int i, int j) { // 调用minDist(n-1, n-1);
    if (i == 0 && j == 0) return matrix[0][0];
    if (mem[i][j] > 0) return mem[i][j];
    int minLeft = Integer.MAX_VALUE;
    if (j-1 >= 0) {
      minLeft = minDist(i, j-1);
    }
    int minUp = Integer.MAX_VALUE;
    if (i-1 >= 0) {
      minUp = minDist(i-1, j);
    }
    
    int currMinDist = matrix[i][j] + Math.min(minLeft, minUp);
    mem[i][j] = currMinDist;
    return currMinDist;
  }
  ```

  两种动态规划解题思路到这里就讲完了。我要强调一点，不是每个问题都同时适合这两种解题思路。有的问题可能用第一种思路更清晰，而有的问题可能用第二种思路更清晰，所以，你要结合具体的题目来看，到底选择用哪种解题思路。



### 四种算法思想比较分析

到今天为止，我们已经学习了四种算法思想**，贪心、分治、回溯和动态规划**。

今天的内容主要讲些理论知识，我正好一块儿也分析一下这四种算法，看看它们之间有什么区别和联系。

如果我们将这四种算法思想分一下类，那**贪心、回溯、动态规划可以归为一类，而分治单独可以作为一类**，因为它跟其他三个都不大一样。为什么这么说呢？前三个算法解决问题的模型，**都可以抽象成我们今天讲的那个多阶段决策最优解模型，而分治算法解决的问题尽管大部分也是最优解问题，但是，大部分都不能抽象成多阶段决策模型。**

​		回溯算法是个“万金油”。基本上能用的动态规划、贪心解决的问题，我们都可以用回溯算法解决。**回溯算法相当于穷举搜索。穷举所有的情况，然后对比得到最优解。不过，回溯算法的时间复杂度非常高，是指数级别的，只能用来解决小规模数据的问题。对于大规模数据的问题，用回溯算法解决的执行效率就很低了。**

​		尽管动态规划比回溯算法高效，但是，并不是所有问题，都可以用动态规划来解决。能用动态规划解决的问题，需要满足三个特征，最优子结构、无后效性和重复子问题。在重复子问题这一点上，**动态规划和分治算法的区分非常明显。分治算法要求分割成的子问题，不能有重复子问题，而动态规划正好相反，动态规划之所以高效，就是因为回溯算法实现中存在大量的重复子问题。**

​		贪心算法实际上是动态规划算法的一种特殊情况。它解决问题起来更加高效，代码实现也更加简洁。不过，它可以解决的问题也更加有限。它能解决的问题需要满足三个条件，最优子结构、无后效性和贪心选择性（这里我们不怎么强调重复子问题）。其中，最优子结构、无后效性跟动态规划中的无异。“贪心选择性”的意思是，通过局部最优的选择，能产生全局的最优选择。每一个阶段，我们都选择当前看起来最优的决策，所有阶段的决策完成之后，最终由这些局部最优解构成全局最优解。





### 动态规划实战：如何实现搜索引擎中的拼写纠错功能？

​		计算机只认识数字，如何量化两个字符串之间的相似程度呢？有一个非常著名的量化方法，那就是**编辑距离（Edit Distance）**。

​		顾名思义，编辑距离指的就是，将一个字符串转化成另一个字符串，需要的最少编辑操作次数（比如增加一个字符、删除一个字符、替换一个字符）。**编辑距离越大，说明两个字符串的相似程度越小；相反，编辑距离就越小，说明两个字符串的相似程度越大。**对于两个完全相同的字符串来说，编辑距离就是 0。

​		根据所包含的编辑操作种类的不同，编辑距离有多种不同的计算方式，比较著名的有**莱文斯坦距离**（**`Levenshtein distance`**）和**最长公共子串长度**（Longest common substring length）。其中，**莱文斯坦距离允许增加、删除、替换字符这三个编辑操作，最长公共子串长度只允许增加、删除字符这两个编辑操作。**

​		而且，莱文斯坦距离和最长公共子串长度，从两个截然相反的角度，分析字符串的相似程度。**莱文斯坦距离的大小，表示两个字符串差异的大小；而最长公共子串的大小，表示两个字符串相似程度的大小。**关于这两个计算方法，我举个例子给你说明一下。这里面，两个字符串 `mitcmu` 和 `mtacnu` 的莱文斯坦距离是 3，最长公共子串长度是 4。

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\字符串相似程度.webp" style="zoom:50%;" />

了解了编辑距离的概念之后，我们来看，**如何快速计算两个字符串之间的编辑距离**？

#### 如何编程计算莱文斯坦距离？

​		这个问题是求把一个字符串变成另一个字符串，需要的最少编辑次数。整个求解过程，涉及多个决策阶段，我们需要依次考察一个字符串中的每个字符，跟另一个字符串中的字符是否匹配，匹配的话如何处理，不匹配的话又如何处理。**所以，这个问题符合多阶段决策最优解模型。**

​		我们前面讲了，贪心、回溯、动态规划可以解决的问题，都可以抽象成这样一个模型。要解决这个问题，我们可以先看一看，用最简单的回溯算法，该如何来解决。**回溯是一个递归处理的过程。**如果 **`a[i]`**与 **`b[j]`**匹配，我们递归考察 **`a[i+1]`**和 **`b[j+1]`**。如果 **`a[i]`**与 **`b[j]`**不匹配，那我们有多种处理方式可选：

- 可以删除 **`a[i]`**，然后递归考察 **`a[i+1]`**和 **`b[j]`**；
- 可以删除 **`b[j]`**，然后递归考察 **`a[i]`**和 **`b[j+1]`**；
- 可以在 **`a[i]`**前面添加一个跟 **`b[j]`**相同的字符，然后递归考察 **`a[i]`**和 **`b[j+1]`**;
- 可以在 **`b[j]`**前面添加一个跟 **`a[i]`**相同的字符，然后递归考察 **`a[i+1]`**和 **`b[j]`**；
- 可以将 **`a[i]`**替换成 **`b[j]`**，或者将 **`b[j]`**替换成 **`a[i]`**，然后递归考察 **`a[i+1]`**和 **`b[j+1]`**。

```java
private char[] a = "mitcmu".toCharArray();
private char[] b = "mtacnu".toCharArray();
private int n = 6;
private int m = 6;
private int minDist = Integer.MAX_VALUE; // 存储结果
// 调用方式 lwstBT(0, 0, 0);
public lwstBT(int i, int j, int edist) {
  if (i == n || j == m) {
    if (i < n) edist += (n-i);
    if (j < m) edist += (m - j);
    if (edist < minDist) minDist = edist;
    return;
  }
  if (a[i] == b[j]) { // 两个字符匹配
    lwstBT(i+1, j+1, edist);
  } else { // 两个字符不匹配
    lwstBT(i + 1, j, edist + 1); // 删除a[i]或者b[j]前添加一个字符
    lwstBT(i, j + 1, edist + 1); // 删除b[j]或者a[i]前添加一个字符
    lwstBT(i + 1, j + 1, edist + 1); // 将a[i]和b[j]替换为相同字符
  }
}
```

​		根据回溯算法的代码实现，我们可以画出递归树，看是否存在重复子问题。**如果存在重复子问题，那我们就可以考虑能否用动态规划来解决；如果不存在重复子问题，那回溯就是最好的解决方法。**

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\字符匹配递归树.webp" style="zoom:50%;" />

在递归树中，每个节点代表一个状态，状态包含三个变量 **`(i, j, edist)`**。

​		其中，**`edist`** 表示处理到 **`a[i]`**和 **`b[j]`**时，已经执行的编辑操作的次数。

​		在递归树中，**`(i, j)`** 两个变量重复的节点很多，比如 (3, 2) 和 (2, 3)。对于 **`(i, j)`** 相同的节点，我们只需要保留 **`edist`** 最小的，继续递归处理就可以了，剩下的节点都可以舍弃。

​		所以，状态就从 **`(i, j, edist)`** 变成了 **`(i, j, min_edist)`**，其中 **`min_edist`** 表示处理到 **`a[i]`**和 **`b[j]`**，已经执行的最少编辑次数。看到这里，你有没有觉得，这个问题跟上两节讲的动态规划例子非常相似？不过，这个问题的状态转移方式，要比之前两节课中讲到的例子都要复杂很多。

​		上一节我们讲的矩阵最短路径问题中，到达状态 **`(i, j)`** 只能通过 **`(i-1, j)`** 或 **`(i, j-1)`** 两个状态转移过来，而今天这个问题，状态 **`(i, j)`** 可能从 **`(i-1, j)`**，**`(i, j-1)`**，**`(i-1, j-1)`** 三个状态中的任意一个转移过来。

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\状态转换.webp" style="zoom:50%;" />

基于刚刚的分析，我们可以尝试着将把状态转移的过程，用公式写出来。这就是我们前面讲的状态转移方程。

```
如果：a[i]!=b[j]，那么：min_edist(i, j)就等于：
min(min_edist(i-1,j)+1, min_edist(i,j-1)+1, min_edist(i-1,j-1)+1)

如果：a[i]==b[j]，那么：min_edist(i, j)就等于：
min(min_edist(i-1,j)+1, min_edist(i,j-1)+1，min_edist(i-1,j-1))

其中，min表示求三数中的最小值。     
```

<img src="C:\Users\Hud\Desktop\文件（需要）\数据结构--Go语言\笔记\递推关系.webp" style="zoom:50%;" />

 我们现在既有状态转移方程，又理清了完整的填表过程，代码实现就非常简单了。我将代码贴在下面，你可以对比着文字解释，一起看下。

```java
public int lwstDP(char[] a, int n, char[] b, int m) {
  int[][] minDist = new int[n][m];
  for (int j = 0; j < m; ++j) { // 初始化第0行:a[0..0]与b[0..j]的编辑距离
    if (a[0] == b[j]) minDist[0][j] = j;
    else if (j != 0) minDist[0][j] = minDist[0][j-1]+1;
    else minDist[0][j] = 1;
  }
  for (int i = 0; i < n; ++i) { // 初始化第0列:a[0..i]与b[0..0]的编辑距离
    if (a[i] == b[0]) minDist[i][0] = i;
    else if (i != 0) minDist[i][0] = minDist[i-1][0]+1;
    else minDist[i][0] = 1;
  }
  for (int i = 1; i < n; ++i) { // 按行填表
    for (int j = 1; j < m; ++j) {
      if (a[i] == b[j]) minDist[i][j] = min(
          minDist[i-1][j]+1, minDist[i][j-1]+1, minDist[i-1][j-1]);
      else minDist[i][j] = min(
          minDist[i-1][j]+1, minDist[i][j-1]+1, minDist[i-1][j-1]+1);
    }
  }
  return minDist[n-1][m-1];
}

private int min(int x, int y, int z) {
  int minv = Integer.MAX_VALUE;
  if (x < minv) minv = x;
  if (y < minv) minv = y;
  if (z < minv) minv = z;
  return minv;
}
```

```go
func lwstDP(a []string, n int, b []string, m int) int {
	var minDist [][]int
	for i := 0; i < n; i++ {
		ar := make([]int, m)
		minDist = append(minDist, ar)
	}
	for j := 0; j < m; j++ { // 初始化第0行:a[0..0]与b[0..j]的编辑距离
		if a[0] == b[j] {
			minDist[0][j] = j
		} else if j != 0 {
			minDist[0][j] = minDist[0][j-1] + 1
		} else {
			minDist[0][j] = 1
		}
	}
	for i := 0; i < n; i++ { // 初始化第0列:a[0..i]与b[0..0]的编辑距离
		if a[i] == b[0] {
			minDist[i][0] = i
		} else if i != 0 {
			minDist[i][0] = minDist[i-1][0] + 1
		} else {
			minDist[i][0] = 1
		}
	}
	for i := 1; i < n; i++ { // 按行填表
		for j := 1; j < m; j++ {
			if a[i] == b[j] {
				minDist[i][j] = min1(minDist[i-1][j]+1, minDist[i][j-1]+1, minDist[i-1][j-1])
			} else {
				minDist[i][j] = min1(minDist[i-1][j]+1, minDist[i][j-1]+1, minDist[i-1][j-1]+1)
			}
		}
	}
	return minDist[n-1][m-1]
}

func min1(x, y, z int) int {
	minV := math.MaxInt64
	if x < minV {
		minV = x
	}
	if y < minV {
		minV = y
	}
	if z < minV {
		minV = z
	}
	return minV
}

```















sad



